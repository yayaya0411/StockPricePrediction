{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZxF0qgXlXihs"
   },
   "source": [
    "# Program Trading Simulation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "j9IK7TeCVmIz"
   },
   "outputs": [],
   "source": [
    "colab = False\n",
    "skl = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yWdIDzwR7Qwr"
   },
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j7vdasm36wGF",
    "outputId": "24f73c9f-2351-4d00-e4e4-7ef25bdaa069"
   },
   "outputs": [],
   "source": [
    "from config import *\n",
    "\n",
    "if colab:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    !cp /content/drive/MyDrive/NCCU/Paper/Program_Trading/TechIndex.py .\n",
    "    !wget https://launchpad.net/~mario-mariomedina/+archive/ubuntu/talib/+files/libta-lib0_0.4.0-oneiric1_amd64.deb -qO libta.deb\n",
    "    !wget https://launchpad.net/~mario-mariomedina/+archive/ubuntu/talib/+files/ta-lib0-dev_0.4.0-oneiric1_amd64.deb -qO ta.deb\n",
    "    !dpkg -i libta.deb ta.deb\n",
    "    !pip install ta-lib\n",
    "    !pip install pandas_datareader --upgrade\n",
    "    !pip install tensorflow_addons\n",
    "    gd_root = '/content/drive/MyDrive/NCCU/Paper/Program_Trading'\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import warnings\n",
    "import datetime\n",
    "import requests\n",
    "import pandas_datareader as pdr\n",
    "from importlib import reload\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "if skl:\n",
    "    import tensorflow as tf\n",
    "    physical_devices = tf.config.list_physical_devices('GPU')\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n",
    "\n",
    "import utility.techIndex\n",
    "reload(utility.techIndex)\n",
    "from utility.techIndex import *\n",
    "import utility.datatools\n",
    "reload(utility.datatools)\n",
    "from utility.datatools import *\n",
    "import utility.structure\n",
    "reload(utility.structure)\n",
    "from utility.structure import *\n",
    "import utility.training\n",
    "reload(utility.training)\n",
    "from utility.training import *\n",
    "import utility.strategy\n",
    "reload(utility.strategy)\n",
    "from utility.strategy import *\n",
    "import config\n",
    "reload(config)\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "awTCiHGDCBTw"
   },
   "source": [
    "## Package Config Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "8cEBZg5yCBoH"
   },
   "outputs": [],
   "source": [
    "# pandas setting\n",
    "# pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('max_colwidth',50)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "pd.options.mode.chained_assignment = None # ignore pandas warning\n",
    "\n",
    "# numpy setting\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "# warning setting\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JzkCWUq0Lw6m"
   },
   "source": [
    "# Model Training Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Worknode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocess(stock_num,train_start,train_end,feature,train=True,scaler_x=None,scaler_y=None):\n",
    "    X = []\n",
    "# read csv\n",
    "    df = pd.read_csv(os.path.join(gd_root,stock_root,stock_num+'.csv'), index_col=0)\n",
    "    df = df.loc[train_start:train_end]\n",
    "\n",
    "# technical indicator\n",
    "    df = talib_index(df)\n",
    "\n",
    "# filter input feature\n",
    "    df = df[feature]\n",
    "\n",
    "# exponential smoothing\n",
    "    # for columnName in df.columns:\n",
    "    #     df[str(columnName)] = exponential_smoothing(5, 0.2, list(df[str(columnName)]))\n",
    "\n",
    "    # df, df_y = y_price(df, days = days)\n",
    "    df, df_y = label(df)\n",
    "\n",
    "    # make lable y\n",
    "    # y_ori = df_y[time_slide:].values\n",
    "    y_ori = df_y[time_slide:]\n",
    "\n",
    "    # for column in new_df.columns:\n",
    "    #     new_df = normalization(new_df)\n",
    "\n",
    "    # use Sclaer to scaler both x & y\n",
    "    if train:\n",
    "        scaler_x = MinMaxScaler()\n",
    "        scaler_y = MinMaxScaler()\n",
    "        scaler_x = scaler_x.fit(df)\n",
    "        # scaler_y = scaler_y.fit(y_ori.reshape(-1, 1))\n",
    "        scaler_y = scaler_y.fit(y_ori)\n",
    "    scaler_df = scaler_x.transform(df)           \n",
    "    # y = scaler_y.transform(y_ori.reshape(-1,1))\n",
    "    y = scaler_y.transform(y_ori)\n",
    "\n",
    "# create training window\n",
    "    for i in range(time_slide, len(scaler_df)): \n",
    "        tmp = scaler_df[i-time_slide:i]\n",
    "        # tmp = normalization(tmp)\n",
    "        X.append(tmp)\n",
    "    return X, y, y_ori, scaler_x, scaler_y, df, df_y \n",
    "\n",
    "\n",
    "def training(stock, train_x, train_y, val_x, val_y, day):\n",
    "    model = lstm4_reg_v1(train_x,train_y)\n",
    "    model_name = f'lstm_{stock}_{train_x.shape[1]}x{train_x.shape[2]}_{day}d_trainbest.h5'\n",
    "    \n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        0.001,\n",
    "        decay_steps=10,\n",
    "        decay_rate=0.9,\n",
    "        )\n",
    "    \n",
    "    model.compile(\n",
    "        loss=\"mean_squared_error\", \n",
    "        metrics = [\"mean_squared_error\"], \n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate = lr_schedule)\n",
    "        )\n",
    "    \n",
    "    history = model.fit(\n",
    "        train_x, train_y , \n",
    "        verbose=2, \n",
    "        epochs = 50, \n",
    "        batch_size = batch_size, \n",
    "        validation_data=(val_x, val_y), \n",
    "        callbacks = callback(os.path.join(gd_root,model_root,model_name))\n",
    "        )\n",
    "    \n",
    "    model_history = pd.DataFrame(history.history)\n",
    "    return model, model_name, model_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7KgWp86DvQB1",
    "outputId": "c0d5b8e2-a464-48c5-ed29-701236125106",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021/11/25 13:34:20\n",
      "slide winodw: 30 days, 22 features\n",
      "\n",
      "1 2408 preprocess\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0101s vs `on_train_batch_end` time: 0.3029s). Check your callbacks.\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.00116, saving model to .\\model\\lstm_2408_30x22_7d_trainbest.h5\n",
      "14/14 - 2s - loss: 0.0356 - mean_squared_error: 0.0356 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 2/50\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.00116 to 0.00013, saving model to .\\model\\lstm_2408_30x22_7d_trainbest.h5\n",
      "14/14 - 0s - loss: 0.0073 - mean_squared_error: 0.0073 - val_loss: 1.3459e-04 - val_mean_squared_error: 1.3459e-04\n",
      "Epoch 3/50\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.00013\n",
      "14/14 - 0s - loss: 0.0049 - mean_squared_error: 0.0049 - val_loss: 3.0784e-04 - val_mean_squared_error: 3.0784e-04\n",
      "Epoch 4/50\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.00013 to 0.00012, saving model to .\\model\\lstm_2408_30x22_7d_trainbest.h5\n",
      "14/14 - 0s - loss: 0.0036 - mean_squared_error: 0.0036 - val_loss: 1.1724e-04 - val_mean_squared_error: 1.1724e-04\n",
      "Epoch 5/50\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00012\n",
      "14/14 - 0s - loss: 0.0034 - mean_squared_error: 0.0034 - val_loss: 3.0599e-04 - val_mean_squared_error: 3.0599e-04\n",
      "Epoch 6/50\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00012\n",
      "14/14 - 0s - loss: 0.0033 - mean_squared_error: 0.0033 - val_loss: 1.7159e-04 - val_mean_squared_error: 1.7159e-04\n",
      "Epoch 7/50\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00012\n",
      "14/14 - 0s - loss: 0.0032 - mean_squared_error: 0.0032 - val_loss: 2.1744e-04 - val_mean_squared_error: 2.1744e-04\n",
      "Epoch 8/50\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00012\n",
      "14/14 - 0s - loss: 0.0032 - mean_squared_error: 0.0032 - val_loss: 3.6644e-04 - val_mean_squared_error: 3.6644e-04\n",
      "Epoch 9/50\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00012\n",
      "14/14 - 0s - loss: 0.0031 - mean_squared_error: 0.0031 - val_loss: 2.5220e-04 - val_mean_squared_error: 2.5220e-04\n",
      "Epoch 10/50\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00012\n",
      "14/14 - 0s - loss: 0.0031 - mean_squared_error: 0.0031 - val_loss: 2.6981e-04 - val_mean_squared_error: 2.6981e-04\n",
      "Epoch 11/50\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00012\n",
      "14/14 - 0s - loss: 0.0031 - mean_squared_error: 0.0031 - val_loss: 2.8102e-04 - val_mean_squared_error: 2.8102e-04\n",
      "Epoch 12/50\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.00012\n",
      "14/14 - 0s - loss: 0.0030 - mean_squared_error: 0.0030 - val_loss: 3.2366e-04 - val_mean_squared_error: 3.2366e-04\n",
      "Epoch 13/50\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00012\n",
      "14/14 - 0s - loss: 0.0030 - mean_squared_error: 0.0030 - val_loss: 2.8763e-04 - val_mean_squared_error: 2.8763e-04\n",
      "Epoch 14/50\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00012\n",
      "14/14 - 0s - loss: 0.0029 - mean_squared_error: 0.0029 - val_loss: 2.8835e-04 - val_mean_squared_error: 2.8835e-04\n",
      "Epoch 15/50\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00012\n",
      "14/14 - 0s - loss: 0.0030 - mean_squared_error: 0.0030 - val_loss: 3.2038e-04 - val_mean_squared_error: 3.2038e-04\n",
      "Epoch 16/50\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00012\n",
      "14/14 - 0s - loss: 0.0028 - mean_squared_error: 0.0028 - val_loss: 3.4849e-04 - val_mean_squared_error: 3.4849e-04\n",
      "Epoch 17/50\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00012\n",
      "14/14 - 0s - loss: 0.0030 - mean_squared_error: 0.0030 - val_loss: 3.3885e-04 - val_mean_squared_error: 3.3885e-04\n",
      "Epoch 18/50\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00012\n",
      "14/14 - 0s - loss: 0.0029 - mean_squared_error: 0.0029 - val_loss: 3.0968e-04 - val_mean_squared_error: 3.0968e-04\n",
      "Epoch 19/50\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00012\n",
      "14/14 - 0s - loss: 0.0029 - mean_squared_error: 0.0029 - val_loss: 3.6078e-04 - val_mean_squared_error: 3.6078e-04\n",
      "Epoch 20/50\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00012\n",
      "14/14 - 0s - loss: 0.0029 - mean_squared_error: 0.0029 - val_loss: 3.6344e-04 - val_mean_squared_error: 3.6344e-04\n",
      "Epoch 21/50\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00012\n",
      "14/14 - 0s - loss: 0.0028 - mean_squared_error: 0.0028 - val_loss: 3.3838e-04 - val_mean_squared_error: 3.3838e-04\n",
      "Epoch 22/50\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00012\n",
      "14/14 - 0s - loss: 0.0028 - mean_squared_error: 0.0028 - val_loss: 3.5743e-04 - val_mean_squared_error: 3.5743e-04\n",
      "Epoch 23/50\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00012\n",
      "14/14 - 0s - loss: 0.0027 - mean_squared_error: 0.0027 - val_loss: 3.5330e-04 - val_mean_squared_error: 3.5330e-04\n",
      "Epoch 24/50\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00012\n",
      "14/14 - 0s - loss: 0.0028 - mean_squared_error: 0.0028 - val_loss: 3.2660e-04 - val_mean_squared_error: 3.2660e-04\n",
      "Epoch 25/50\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00012\n",
      "14/14 - 0s - loss: 0.0029 - mean_squared_error: 0.0029 - val_loss: 3.4829e-04 - val_mean_squared_error: 3.4829e-04\n",
      "Epoch 26/50\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00012\n",
      "14/14 - 0s - loss: 0.0028 - mean_squared_error: 0.0028 - val_loss: 3.6648e-04 - val_mean_squared_error: 3.6648e-04\n",
      "Epoch 27/50\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00012\n",
      "14/14 - 0s - loss: 0.0028 - mean_squared_error: 0.0028 - val_loss: 3.6836e-04 - val_mean_squared_error: 3.6836e-04\n",
      "Epoch 28/50\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00012\n",
      "14/14 - 0s - loss: 0.0028 - mean_squared_error: 0.0028 - val_loss: 3.5566e-04 - val_mean_squared_error: 3.5566e-04\n",
      "Epoch 29/50\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00012\n",
      "14/14 - 0s - loss: 0.0027 - mean_squared_error: 0.0027 - val_loss: 3.6738e-04 - val_mean_squared_error: 3.6738e-04\n",
      "Epoch 30/50\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00012\n",
      "14/14 - 0s - loss: 0.0029 - mean_squared_error: 0.0029 - val_loss: 3.6941e-04 - val_mean_squared_error: 3.6941e-04\n",
      "Epoch 31/50\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00012\n",
      "14/14 - 0s - loss: 0.0028 - mean_squared_error: 0.0028 - val_loss: 3.7827e-04 - val_mean_squared_error: 3.7827e-04\n",
      "Epoch 32/50\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00012\n",
      "14/14 - 0s - loss: 0.0028 - mean_squared_error: 0.0028 - val_loss: 3.7114e-04 - val_mean_squared_error: 3.7114e-04\n",
      "Epoch 33/50\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00012\n",
      "14/14 - 0s - loss: 0.0028 - mean_squared_error: 0.0028 - val_loss: 3.6141e-04 - val_mean_squared_error: 3.6141e-04\n",
      "Epoch 34/50\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00012\n",
      "14/14 - 0s - loss: 0.0027 - mean_squared_error: 0.0027 - val_loss: 3.6846e-04 - val_mean_squared_error: 3.6846e-04\n",
      "Epoch 35/50\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00012\n",
      "14/14 - 0s - loss: 0.0027 - mean_squared_error: 0.0027 - val_loss: 3.7080e-04 - val_mean_squared_error: 3.7080e-04\n",
      "Epoch 36/50\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00012\n",
      "14/14 - 0s - loss: 0.0029 - mean_squared_error: 0.0029 - val_loss: 3.7431e-04 - val_mean_squared_error: 3.7431e-04\n",
      "Epoch 37/50\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00012\n",
      "14/14 - 0s - loss: 0.0027 - mean_squared_error: 0.0027 - val_loss: 3.7447e-04 - val_mean_squared_error: 3.7447e-04\n",
      "Epoch 38/50\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00012\n",
      "14/14 - 0s - loss: 0.0028 - mean_squared_error: 0.0028 - val_loss: 3.6939e-04 - val_mean_squared_error: 3.6939e-04\n",
      "Epoch 39/50\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00012\n",
      "14/14 - 0s - loss: 0.0028 - mean_squared_error: 0.0028 - val_loss: 3.6472e-04 - val_mean_squared_error: 3.6472e-04\n",
      "Epoch 40/50\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00012\n",
      "14/14 - 0s - loss: 0.0028 - mean_squared_error: 0.0028 - val_loss: 3.6280e-04 - val_mean_squared_error: 3.6280e-04\n",
      "Epoch 41/50\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00012\n",
      "14/14 - 0s - loss: 0.0027 - mean_squared_error: 0.0027 - val_loss: 3.6444e-04 - val_mean_squared_error: 3.6444e-04\n",
      "Epoch 42/50\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00012\n",
      "14/14 - 0s - loss: 0.0029 - mean_squared_error: 0.0029 - val_loss: 3.6764e-04 - val_mean_squared_error: 3.6764e-04\n",
      "Epoch 43/50\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00012\n",
      "14/14 - 0s - loss: 0.0029 - mean_squared_error: 0.0029 - val_loss: 3.6823e-04 - val_mean_squared_error: 3.6823e-04\n",
      "Epoch 44/50\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00012\n",
      "14/14 - 0s - loss: 0.0027 - mean_squared_error: 0.0027 - val_loss: 3.6903e-04 - val_mean_squared_error: 3.6903e-04\n",
      "Epoch 45/50\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00012\n",
      "14/14 - 0s - loss: 0.0028 - mean_squared_error: 0.0028 - val_loss: 3.6711e-04 - val_mean_squared_error: 3.6711e-04\n",
      "Epoch 46/50\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00012\n",
      "14/14 - 0s - loss: 0.0028 - mean_squared_error: 0.0028 - val_loss: 3.6627e-04 - val_mean_squared_error: 3.6627e-04\n",
      "Epoch 47/50\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00012\n",
      "14/14 - 0s - loss: 0.0027 - mean_squared_error: 0.0027 - val_loss: 3.6651e-04 - val_mean_squared_error: 3.6651e-04\n",
      "Epoch 48/50\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00012\n",
      "14/14 - 0s - loss: 0.0028 - mean_squared_error: 0.0028 - val_loss: 3.6801e-04 - val_mean_squared_error: 3.6801e-04\n",
      "Epoch 49/50\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00012\n",
      "14/14 - 0s - loss: 0.0027 - mean_squared_error: 0.0027 - val_loss: 3.6723e-04 - val_mean_squared_error: 3.6723e-04\n",
      "Epoch 50/50\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00012\n",
      "14/14 - 0s - loss: 0.0028 - mean_squared_error: 0.0028 - val_loss: 3.6794e-04 - val_mean_squared_error: 3.6794e-04\n",
      "Wall time: 22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print_time(colab)\n",
    "status(time_slide, feature)\n",
    "\n",
    "counter = 1\n",
    "train_start = '2001-01-01'\n",
    "train_end = '2017-12-31'\n",
    "stockNum = ['2408']\n",
    "\n",
    "for stock_num in stockNum:\n",
    "    tf.keras.backend.clear_session()\n",
    "    print(counter,str(stock_num),\"preprocess\")\n",
    "\n",
    "    X, y, y_ori, scaler_x, scaler_y, _, _ = data_preprocess(stock_num, train_start, train_end, feature)\n",
    "    train_x, train_y, val_x, val_y, price_y_tra, price_y = input_build(X, y, y_ori)\n",
    "    model, model_name, model_history = training(stock_num, train_x, train_y, val_x, val_y, day=7)\n",
    "    \n",
    "    # Save Model    \n",
    "    # print(model_lt_name)\n",
    "    # model_lt.save(os.path.join(gd_root,model_root,model_lt_name))\n",
    "    counter += 1\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 437
    },
    "id": "OmYsyRp42kTH",
    "outputId": "834ee021-97b4-4833-fb02-6e80ae564526"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021/11/25 13:34:53\n",
      "slide winodw: 30 days, 22 features\n",
      "\n",
      "Wall time: 41 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x175f1177a48>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "print_time(colab)\n",
    "status(time_slide, feature)\n",
    "\n",
    "sns.set(style = 'white',font_scale=1.5)\n",
    "fig, ax = plt.subplots(1,1,figsize=(20, 4))\n",
    "sns.lineplot(x=model_history.index , y = 'loss' ,data = model_history, ax = ax)\n",
    "sns.lineplot(x=model_history.index , y = 'val_loss' ,data = model_history, ax = ax)\n",
    "ax.set_title('Model Loss')\n",
    "ax.set_xlabel ('epoch')\n",
    "ax.set_ylabel ('loss')\n",
    "ax.legend(['train', 'valid'], loc='upper right')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xqgakkl41Ah0"
   },
   "source": [
    "## Model Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 485
    },
    "id": "zsvgqmbHYu66",
    "outputId": "61aeca73-e319-4d91-883c-c352b8a96e44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021/11/25 13:34:57\n",
      "slide winodw: 30 days, 22 features\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Before Inverse</th>\n",
       "      <th>After Inverse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MSE</th>\n",
       "      <td>0.000</td>\n",
       "      <td>517.076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMSE</th>\n",
       "      <td>0.019</td>\n",
       "      <td>22.739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R2</th>\n",
       "      <td>-1.077</td>\n",
       "      <td>-1.077</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Before Inverse  After Inverse\n",
       "MSE            0.000        517.076\n",
       "RMSE           0.019         22.739\n",
       "R2            -1.077         -1.077"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x175d7f6dcc8>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "print_time(colab)\n",
    "status(time_slide, feature)\n",
    "pred_y = model.predict(val_x)\n",
    "pred_y_inverse = scaler_y.inverse_transform(pred_y)\n",
    "# Metric\n",
    "metric = {\n",
    "    'Before Inverse':[\n",
    "        mean_squared_error(val_y, pred_y, squared = True),\n",
    "        mean_squared_error(val_y, pred_y, squared = False),\n",
    "        r2_score(val_y, pred_y)\n",
    "    ],\n",
    "    'After Inverse':[\n",
    "        mean_squared_error(price_y, pred_y_inverse, squared = True),\n",
    "        mean_squared_error(price_y, pred_y_inverse, squared = False),\n",
    "        r2_score(price_y, pred_y_inverse)\n",
    "    ]\n",
    " }\n",
    "df_metric = pd.DataFrame(metric,index=['MSE','RMSE','R2'])\n",
    "display(df_metric)\n",
    "\n",
    "df = pd.DataFrame(\n",
    "            {\n",
    "        'price_3d':price_y['y_3d'], \n",
    "        'price_5d':price_y['y_5d'], \n",
    "        'pred_3d': [x[0] for x in pred_y_inverse], \n",
    "        'pred_5d':[x[1] for x in pred_y_inverse]\n",
    "        }\n",
    "    )\n",
    "# display(df)\n",
    "fig, ax = plt.subplots(2,1,figsize=(20, 10), sharex=True)\n",
    "fig.suptitle('Valid Period')\n",
    "sns.set(style = 'white',font_scale=1.5)\n",
    "ax[0].set_title('3 Days Predict Price')\n",
    "sns.lineplot(data = df[['price_3d','pred_3d']], ax = ax[0])\n",
    "ax[1].set_title('5 Days Predict Price')\n",
    "sns.lineplot(data = df[['price_5d','pred_5d']], ax = ax[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x17581fa0d48>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1080 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_y = model.predict(train_x)\n",
    "pred_y_inverse = scaler_y.inverse_transform(pred_y)\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {       \n",
    "    'price_3d':price_y_tra['y_3d'], \n",
    "    'price_5d':price_y_tra['y_5d'], \n",
    "    'pred_3d':[x[0] for x in pred_y_inverse],\n",
    "    'pred_5d':[x[1] for x in pred_y_inverse],\n",
    "    })\n",
    "# df_lt = pd.DataFrame({'price_lt':price_y_tra, 'pred_lt':pred_y_lt_inverse.reshape(len(pred_y_lt_inverse))})\n",
    "fig, ax = plt.subplots(2,1,figsize=(20, 15), sharex=True)\n",
    "fig.suptitle('Train Period')\n",
    "ax[0].set_title('3 Days Predict Price')\n",
    "sns.set(style = 'white',font_scale=1.5)\n",
    "sns.lineplot(data = df[['price_3d','pred_3d']],ax=ax[0])\n",
    "ax[1].set_title('5 Days Predict Price')\n",
    "sns.lineplot(data = df[['price_5d','pred_5d']], ax = ax[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vO9NlUijL54o"
   },
   "source": [
    "# Trading Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XVcoawlrZQ48"
   },
   "source": [
    "## Test Period Rate of Return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 538
    },
    "id": "W9UOw2MtZXPW",
    "outputId": "b6854e06-5748-4d90-cb54-79c0699ef7b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2412 preprocess\n",
      "00 2412 最後資產： 1066187 交易報酬率： 6.6187 % 持有報酬率 2.3256 %\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "If using all scalar values, you must pass an index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    612\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    613\u001b[0m             \u001b[1;31m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 614\u001b[1;33m             \u001b[0mmgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict_to_mgr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmanager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    615\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    616\u001b[0m             \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    462\u001b[0m         \u001b[1;31m# TODO: can we get rid of the dt64tz special case above?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 464\u001b[1;33m     return arrays_to_mgr(\n\u001b[0m\u001b[0;32m    465\u001b[0m         \u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtyp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconsolidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    466\u001b[0m     )\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, arr_names, index, columns, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[1;31m# figure out the index, if necessary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m             \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_extract_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    623\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    624\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mindexes\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mraw_lengths\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 625\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"If using all scalar values, you must pass an index\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    626\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhave_series\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: If using all scalar values, you must pass an index"
     ]
    }
   ],
   "source": [
    "\n",
    "%%time\n",
    "# setting backtest model\n",
    "\n",
    "train_start = '2018-01-01'\n",
    "train_end = '2019-12-31'\n",
    "# stockNum = ['2412']\n",
    "# setting the information of data\n",
    "asset_at_the_start = 1000000\n",
    "total_asset_present_value = 0\n",
    "record_ror = []\n",
    "trading_ROR = {}\n",
    "portfolio_list = []\n",
    "compare_ror = []\n",
    "counter = 0\n",
    "\n",
    "for stock_num in stockNum:\n",
    "    tf.keras.backend.clear_session()\n",
    "    print(counter,str(stock_num),\"preprocess\")\n",
    "    # Prepare Data\n",
    "    # todo modify data preprocess, separate scaler & long short terms \n",
    "    day = 1\n",
    "    X, y, y_ori, _, _, df, df_y = data_preprocess(stock_num, train_start, train_end, feature, days = day, train=False,scaler_x=scaler_x_st,scaler_y=scaler_y_st)\n",
    "\n",
    "# start backtest evaluaion \n",
    "    left_money = asset_at_the_start*(1/len(stockNum))\n",
    "    own_asset = left_money \n",
    "    own_stock = 0 \n",
    "    # money_record = [left_money]*time_slide\n",
    "    stock_record = [[x,left_money] for x in df.index[:time_slide]]\n",
    "    # origin_hold_stock = 0 \n",
    "    X = np.array(X)\n",
    "    day_st = 3\n",
    "    day_lt = 5\n",
    "    model_st_name = f'lstm_{stock_num}_{X.shape[1]}x{X.shape[2]}_{day_st}d_trainbest.h5'\n",
    "    model_lt_name = f'lstm_{stock_num}_{X.shape[1]}x{X.shape[2]}_{day_lt}d_trainbest.h5'\n",
    "    model_st = tf.keras.models.load_model(os.path.join(gd_root,model_root,model_st_name))\n",
    "    model_lt = tf.keras.models.load_model(os.path.join(gd_root,model_root,model_lt_name))\n",
    "    # print(os.path.join(gd_root,model_root,model_st_name))\n",
    "    # print(os.path.join(gd_root,model_root,model_lt_name))\n",
    "    df_trade = df[time_slide:]\n",
    "    for window in range(len(X)):\n",
    "#         print(model_st.predict(X[window]))\n",
    "        price_s = scaler_y_st.inverse_transform(model_st.predict(X[window][np.newaxis,:]))\n",
    "        price_l = scaler_y_lt.inverse_transform(model_lt.predict(X[window][np.newaxis,:]))\n",
    "        predict = (price_s,price_l)\n",
    "        # predict = (model_st.predict(X[window][np.newaxis,:], model_lt.predict(X[window][np.newaxis,:]))\n",
    "        price = df_trade['Close'][window]\n",
    "        stock_assets, own_stock, left_money, action, pred, price = highlow_strategy(price_l, own_stock, left_money, price)\n",
    "        stock_record.append([df_trade.index[window], stock_assets, action, pred, price])\n",
    "    \n",
    "    daily_assets = pd.DataFrame(stock_record, columns = ['date','assets','action','pred_price','close_price']).set_index('date')  \n",
    "    portfolio_list.append(daily_assets)\n",
    "    last_money = stock_record[-1][1]\n",
    "    left_assets = int(last_money)-(own_asset)\n",
    "    record_ror.append([str(stock_num), int(last_money), left_assets/own_asset])\n",
    "    \n",
    "    # compare to buy & hold strategy\n",
    "    bench_df = pdr.DataReader(str(stock_num)+'.tw', 'yahoo', start=start, end=end)\n",
    "    buyhold_ror = ((bench_df['Close'][-1]/bench_df['Close'][0])-1)*100\n",
    "    stock_ror = (left_assets*100/(own_asset))\n",
    "\n",
    "    print(f\"{str(counter).zfill(2)} {str(stock_num)} 最後資產： {int(last_money)} 交易報酬率： {stock_ror} % 持有報酬率 {buyhold_ror:.4f} %\")\n",
    "    \n",
    "    total_asset_present_value += int(last_money)\n",
    "    trading_ROR[stock_num] = stock_ror\n",
    "    compare_ror.append([stock_num, stock_ror, buyhold_ror])\n",
    "\n",
    "trading_df = pd.DataFrame(compare_ror,columns=['stock','trading ror','buyhold ror'])\n",
    "\n",
    "print(\"\\n總投資報酬率：\", (total_asset_present_value - asset_at_the_start)*100 / asset_at_the_start, \"%\\n\")\n",
    "\n",
    "Valid_Portfolio = portfolio_list[0].join(portfolio_list[1:])\n",
    "Valid_Portfolio['total'] = Valid_Portfolio.sum(axis=1)\n",
    "save_pickle(Valid_Portfolio, os.path.join(gd_root, ror_root, 'Valid_Portfolio.pkl'))\n",
    "save_pickle(trading_ROR, os.path.join(gd_root, ror_root, 'trading_ROR.pkl'))\n",
    "ror_df = pd.DataFrame(trading_ROR.items(), columns = ['Stock','trading_ROR'])\n",
    "ror_df.to_csv(os.path.join(gd_root, ror_root, 'trading_ROR.csv'))\n",
    "display(trading_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9k9CW4khv-CY"
   },
   "source": [
    "## Test Portfolio Assets Change\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "gxo0HWqBSiwX"
   },
   "outputs": [],
   "source": [
    "start = '2018/01/01'\n",
    "end = '2019/12/31'\n",
    "# bench_df = loading_data_api(str('2412'), start, end)\n",
    "bench_df = pdr.DataReader('2412.tw', 'yahoo', start=start, end=end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 463
    },
    "id": "b-x5OVNyv8j_",
    "outputId": "2776ee51-6fc6-45a9-e4bd-fd51390de47a"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Valid_Portfolio = load_pickle(os.path.join(gd_root,'Valid_Portfolio.pkl'))\n",
    "Valid_Portfolio.index = pd.to_datetime(Valid_Portfolio.index)\n",
    "Valid_Portfolio['total_ratio'] = (Valid_Portfolio['total']/Valid_Portfolio['total'][0])*100\n",
    "Valid = Valid_Portfolio[time_slide:]\n",
    "Benchmark = bench_df[time_slide:-1]\n",
    "Benchmark.index = pd.to_datetime(Benchmark.index)\n",
    "Benchmark['ratio'] = (Benchmark['Close']/Benchmark['Close'][0])*100\n",
    "TWII = twii_df[time_slide:-1]\n",
    "TWII.index = pd.to_datetime(TWII.index)\n",
    "TWII['ratio'] = (TWII['Close']/TWII['Close'][0])*100\n",
    "\n",
    "sns.set(style = 'white',font_scale=1.5)\n",
    "fig, ax = plt.subplots(figsize=(12, 6), sharex=True)\n",
    "# fig.suptitle('Valid Portfolio Assets Change')\n",
    "sns.lineplot(data = Valid,x=Valid.index,y='total_ratio',label=\"Trading\")\n",
    "sns.lineplot(data = Benchmark, x =Benchmark.index, y = 'ratio', label=\"2412\")\n",
    "# sns.lineplot(data = TWII, x = TWII.index, y = 'ratio', label=\"TWII\")\n",
    "\n",
    "ax.axhline(100, ls='--',color='red')\n",
    "plt.tight_layout(pad=0)\n",
    "# plt.savefig('Valid_Portfolio Assets Change.svg', dpi=300)\n",
    "# plt.savefig('Valid_Portfolio Assets Change.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4O7IH3SDOai3"
   },
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 454
    },
    "id": "1RXdwNGKL8AI",
    "outputId": "88c9a953-be36-4378-dedc-7dd84c0dd3a2"
   },
   "outputs": [],
   "source": [
    "sns.set(style=\"white\", palette=\"muted\", color_codes=True)\n",
    "fig, ax = plt.subplots(figsize=(12, 6), sharex=True)\n",
    "fig.suptitle('MarginPurchaseBuy Original')\n",
    "sns.histplot(bench_df['MarginPurchaseBuy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 454
    },
    "id": "tJbWT7zfMGv8",
    "outputId": "d6f0d3ed-e14e-4ebc-d514-1c6b68a9ff75"
   },
   "outputs": [],
   "source": [
    "sns.set(style=\"white\", palette=\"muted\", color_codes=True)\n",
    "fig, ax = plt.subplots(figsize=(12, 6), sharex=True)\n",
    "fig.suptitle('MarginPurchaseBuy Normalized')\n",
    "a = bench_df['MarginPurchaseBuy'].mean()\n",
    "b = bench_df['MarginPurchaseBuy'].std()\n",
    "sns.histplot(bench_df['MarginPurchaseBuy'].apply(lambda x : (x-a)/b))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GG8ibN8pU5pZ"
   },
   "source": [
    "## Smoothing Data Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "id": "VFLLOH0tfviR",
    "outputId": "7d204b1c-608b-45e4-bf62-653686ffaff9"
   },
   "outputs": [],
   "source": [
    "new_df # expional smooth 過\n",
    "new_df_ori # 原始\n",
    "\n",
    "draw = 'Close'\n",
    "df = pd.concat([new_df[draw], new_df_ori[draw]], axis=1)\n",
    "df.columns=['exp','ori']\n",
    "df = df.iloc[-200:]\n",
    "# display(df.tail())\n",
    "\n",
    "sns.set(style=\"white\", palette=\"muted\", color_codes=True, font_scale=1.6)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5), sharex=True)\n",
    "# fig.suptitle(f'Original & Smoothing {draw}')\n",
    "sns.lineplot(data=df, x=df.index, y='ori', label = 'Original', color='g' ,linestyle='--' ,ax=ax)\n",
    "sns.lineplot(data=df, x=df.index, y='exp', label = 'Exponential', ax=ax)\n",
    "# plt.marlegendgins(x=0,y=0)\n",
    "# plt.(fontsize = 15)\n",
    "plt.ticklabel_format(style='plain', axis='y')\n",
    "plt.xlabel(\"Days\")\n",
    "plt.ylabel(f\"{draw}\")\n",
    "plt.tight_layout(pad=0)\n",
    "plt.savefig('smoothing.svg', dpi=300)\n",
    "plt.savefig('smoothing.jpg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021/11/12 13:18:50\n",
      "slide winodw: 5 days, 22 features, predict 1 days Close price\n",
      "\n",
      "1 1101 preprocess\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.08701, saving model to .\\model\\lstm_1101_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.08701 to 0.02600, saving model to .\\model\\lstm_1101_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.02600 to 0.00685, saving model to .\\model\\lstm_1101_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.00685\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00685\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00685\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00685 to 0.00571, saving model to .\\model\\lstm_1101_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00571 to 0.00383, saving model to .\\model\\lstm_1101_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.11595, saving model to .\\model\\lstm_1101_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.11595 to 0.06087, saving model to .\\model\\lstm_1101_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.06087 to 0.01064, saving model to .\\model\\lstm_1101_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.01064\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.01064\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.01064\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.01064 to 0.00924, saving model to .\\model\\lstm_1101_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00924 to 0.00444, saving model to .\\model\\lstm_1101_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00444\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00444\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00444\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.00444\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00444\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00444\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00444\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00444\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00444\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.00444 to 0.00342, saving model to .\\model\\lstm_1101_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.00342 to 0.00180, saving model to .\\model\\lstm_1101_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00180\n",
      "1 1101 trading\n",
      "01 1101 最後資產： 21389 交易報酬率： 0.5283000000000062 % 持有報酬率 40.7391 %\n",
      "2 1216 preprocess\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.38077, saving model to .\\model\\lstm_1216_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.38077 to 0.22238, saving model to .\\model\\lstm_1216_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.22238 to 0.05934, saving model to .\\model\\lstm_1216_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.05934 to 0.01475, saving model to .\\model\\lstm_1216_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.01475 to 0.01341, saving model to .\\model\\lstm_1216_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.01341\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.01341\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.01341\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.01341\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.01341 to 0.00694, saving model to .\\model\\lstm_1216_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00694 to 0.00604, saving model to .\\model\\lstm_1216_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.00604\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00604\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.00604 to 0.00565, saving model to .\\model\\lstm_1216_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00565 to 0.00395, saving model to .\\model\\lstm_1216_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00395\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00395\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00395\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00395\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.00395 to 0.00363, saving model to .\\model\\lstm_1216_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00363\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00363\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00363\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00363\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00363\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00363\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00363\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00363\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00363\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00363\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00363\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00363\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00363\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00363\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00363\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00363\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00363\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00363\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00363\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00363\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00363\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00363\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00363\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00363\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00363\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00363\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00363\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00363\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00363\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00363\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00363\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00363\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00363\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00363\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00363\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00363\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00363\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00363\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00363\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00363\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00363\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00363\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00363\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00363\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00363\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00363\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00363\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00363\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00363\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00363\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00363\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00363\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00363\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00363\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00363\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00363\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00363\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00363\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00363\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00363\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00363\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00363\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00363\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00363\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00363\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00363\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00363\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00363\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00363\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00363\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00363\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00363\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00363\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00363\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00363\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00363\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00363\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00363\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00363\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00363\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.40718, saving model to .\\model\\lstm_1216_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.40718 to 0.29559, saving model to .\\model\\lstm_1216_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.29559 to 0.15740, saving model to .\\model\\lstm_1216_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.15740 to 0.04057, saving model to .\\model\\lstm_1216_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.04057 to 0.01652, saving model to .\\model\\lstm_1216_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.01652\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.01652\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.01652\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.01652\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.01652\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.01652 to 0.01036, saving model to .\\model\\lstm_1216_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.01036 to 0.00826, saving model to .\\model\\lstm_1216_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00826\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00826\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00826 to 0.00782, saving model to .\\model\\lstm_1216_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.00782 to 0.00509, saving model to .\\model\\lstm_1216_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00509\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00509\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00509\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00509\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00509\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00509\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00509\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.00509 to 0.00503, saving model to .\\model\\lstm_1216_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00503\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00503\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00503\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00503\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00503\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00503\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00503\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00503\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00503\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00503\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00503\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00503\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00503\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00503\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00503\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.00503 to 0.00494, saving model to .\\model\\lstm_1216_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00494\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00494\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00494\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00494\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00494\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00494\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00494\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00494\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00494\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00494\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00494\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00494\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00494\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00494\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00494\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00494\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00494\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00494\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00494\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00494\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00494\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00494\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00494\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00494\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00494\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00494\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00494\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00494\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00494\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00494\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00494\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00494\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00494\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00494\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00494\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00494\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00494\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00494\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00494\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00494\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00494\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00494\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00494\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00494\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00494\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00494\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00494\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00494\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00494\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00494\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00494\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00494\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00494\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00494\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00494\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00494\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00494\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00494\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00494\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00494\n",
      "2 1216 trading\n",
      "02 1216 最後資產： 21855 交易報酬率： 2.7185000000000064 % 持有報酬率 10.4167 %\n",
      "3 1301 preprocess\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.52981, saving model to .\\model\\lstm_1301_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.52981 to 0.34884, saving model to .\\model\\lstm_1301_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.34884 to 0.14203, saving model to .\\model\\lstm_1301_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.14203 to 0.03677, saving model to .\\model\\lstm_1301_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.03677\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.03677\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.03677\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.03677\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.03677\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.03677\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.03677\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.03677\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.03677\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.03677\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.03677\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.03677\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.03677\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.03677\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.03677 to 0.03602, saving model to .\\model\\lstm_1301_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.03602 to 0.02383, saving model to .\\model\\lstm_1301_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.02383 to 0.01846, saving model to .\\model\\lstm_1301_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.01846 to 0.01651, saving model to .\\model\\lstm_1301_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.01651 to 0.01422, saving model to .\\model\\lstm_1301_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.01422 to 0.01244, saving model to .\\model\\lstm_1301_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.01244 to 0.01125, saving model to .\\model\\lstm_1301_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.01125\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.01125\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.01125\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.01125\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.01125\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.01125\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.01125\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.01125\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.01125 to 0.00852, saving model to .\\model\\lstm_1301_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00852\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00852\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00852\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00852\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00852\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00852\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00852\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00852\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.00852 to 0.00849, saving model to .\\model\\lstm_1301_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.00849 to 0.00817, saving model to .\\model\\lstm_1301_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00817\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.00817 to 0.00565, saving model to .\\model\\lstm_1301_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00565\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00565\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00565\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00565\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00565\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00565\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00565\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00565\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00565\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00565\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00565\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00565\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00565\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00565\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00565\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00565\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00565\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00565\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.00565 to 0.00506, saving model to .\\model\\lstm_1301_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00506\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00506\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00506\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00506\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00506\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00506\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00506\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00506\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00506\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00506\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00506\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00506\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00506\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00506\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00506\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00506\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00506\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00506\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00506\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00506\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00506\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00506\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00506\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00506\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00506\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00506\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00506\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00506\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00506\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.00506 to 0.00449, saving model to .\\model\\lstm_1301_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00449\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00449\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00449\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00449\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00449\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.57444, saving model to .\\model\\lstm_1301_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.57444 to 0.43797, saving model to .\\model\\lstm_1301_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.43797 to 0.25457, saving model to .\\model\\lstm_1301_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.25457 to 0.07651, saving model to .\\model\\lstm_1301_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.07651 to 0.05774, saving model to .\\model\\lstm_1301_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.05774\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.05774\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.05774\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.05774\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.05774\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.05774\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.05774\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.05774\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.05774\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.05774\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.05774\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.05774\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.05774 to 0.05689, saving model to .\\model\\lstm_1301_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.05689 to 0.05553, saving model to .\\model\\lstm_1301_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.05553 to 0.04367, saving model to .\\model\\lstm_1301_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.04367 to 0.02874, saving model to .\\model\\lstm_1301_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.02874 to 0.02260, saving model to .\\model\\lstm_1301_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.02260\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.02260 to 0.01667, saving model to .\\model\\lstm_1301_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.01667 to 0.01587, saving model to .\\model\\lstm_1301_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.01587\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.01587\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.01587 to 0.01472, saving model to .\\model\\lstm_1301_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.01472\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.01472\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.01472 to 0.01390, saving model to .\\model\\lstm_1301_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.01390\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.01390 to 0.01360, saving model to .\\model\\lstm_1301_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.01360 to 0.01084, saving model to .\\model\\lstm_1301_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.01084\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.01084\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.01084\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.01084\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.01084 to 0.01013, saving model to .\\model\\lstm_1301_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.01013\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.01013\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.01013 to 0.00857, saving model to .\\model\\lstm_1301_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00857\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00857\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00857\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00857\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00857\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00857\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00857\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00857\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00857\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.00857 to 0.00853, saving model to .\\model\\lstm_1301_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00853\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00853\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00853\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.00853 to 0.00805, saving model to .\\model\\lstm_1301_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00805\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.00805 to 0.00755, saving model to .\\model\\lstm_1301_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00755\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00755\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.00755 to 0.00751, saving model to .\\model\\lstm_1301_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00751\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00751\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00751\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00751\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00751\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00751\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00751\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.00751 to 0.00547, saving model to .\\model\\lstm_1301_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00547\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00547\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00547\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00547\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00547\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00547\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00547\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00547\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00547\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00547\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00547\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00547\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00547\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00547\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00547\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00547\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00547\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00547\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00547\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00547\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00547\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.00547 to 0.00389, saving model to .\\model\\lstm_1301_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00389\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00389\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00389\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00389\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00389\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00389\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00389\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00389\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00389\n",
      "3 1301 trading\n",
      "03 1301 最後資產： 21965 交易報酬率： 3.2355000000000063 % 持有報酬率 1.7329 %\n",
      "4 1303 preprocess\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.43521, saving model to .\\model\\lstm_1303_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.43521 to 0.21282, saving model to .\\model\\lstm_1303_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.21282 to 0.03304, saving model to .\\model\\lstm_1303_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.03304 to 0.00820, saving model to .\\model\\lstm_1303_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00820\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00820\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00820\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00820\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00820\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00820\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00820\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.00820\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00820\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00820\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00820\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00820\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00820\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00820\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00820\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.00820 to 0.00753, saving model to .\\model\\lstm_1303_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.00753 to 0.00734, saving model to .\\model\\lstm_1303_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00734\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00734\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00734\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00734\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00734\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00734\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00734\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.00734 to 0.00661, saving model to .\\model\\lstm_1303_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00661\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00661\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.00661 to 0.00615, saving model to .\\model\\lstm_1303_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00615\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00615\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00615\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00615\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00615\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00615\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00615\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00615\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00615\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.00615 to 0.00607, saving model to .\\model\\lstm_1303_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00607\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00607\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00607\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00607\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00607\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00607\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00607\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00607\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00607\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00607\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00607\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00607\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00607\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00607\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00607\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00607\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00607\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00607\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00607\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00607\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00607\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00607\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00607\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00607\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00607\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00607\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00607\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00607\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00607\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00607\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00607\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00607\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00607\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00607\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00607\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00607\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00607\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00607\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00607\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00607\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00607\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00607\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00607\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00607\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00607\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00607\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00607\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00607\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00607\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00607\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00607\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00607\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00607\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00607\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00607\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00607\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00607\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00607\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.49994, saving model to .\\model\\lstm_1303_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.49994 to 0.30359, saving model to .\\model\\lstm_1303_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.30359 to 0.08888, saving model to .\\model\\lstm_1303_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.08888 to 0.00998, saving model to .\\model\\lstm_1303_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00998\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00998\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00998\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00998\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00998\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00998\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00998\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.00998\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00998\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00998\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00998\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00998\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00998\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00998\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00998\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.00998 to 0.00979, saving model to .\\model\\lstm_1303_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.00979 to 0.00868, saving model to .\\model\\lstm_1303_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00868\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00868\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00868\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00868\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00868\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00868\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00868\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00868\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00868\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.00868 to 0.00797, saving model to .\\model\\lstm_1303_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.00797 to 0.00763, saving model to .\\model\\lstm_1303_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00763\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00763\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00763\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00763\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00763\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00763\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00763\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00763\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00763\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00763\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.00763 to 0.00631, saving model to .\\model\\lstm_1303_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00631\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00631\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00631\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00631\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00631\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00631\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00631\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00631\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00631\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00631\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00631\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00631\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00631\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00631\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00631\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00631\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00631\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00631\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00631\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00631\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00631\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00631\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00631\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00631\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00631\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00631\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00631\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00631\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00631\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00631\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00631\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00631\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00631\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00631\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00631\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00631\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00631\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00631\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00631\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00631\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00631\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00631\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00631\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00631\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00631\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00631\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00631\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00631\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00631\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00631\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00631\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00631\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00631\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00631\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00631\n",
      "4 1303 trading\n",
      "04 1303 最後資產： 20298 交易報酬率： -4.599399999999994 % 持有報酬率 -5.4545 %\n",
      "5 1326 preprocess\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.49528, saving model to .\\model\\lstm_1326_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.49528 to 0.34955, saving model to .\\model\\lstm_1326_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.34955 to 0.18075, saving model to .\\model\\lstm_1326_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.18075 to 0.05773, saving model to .\\model\\lstm_1326_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.05773 to 0.05497, saving model to .\\model\\lstm_1326_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.05497\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.05497\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.05497\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.05497\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.05497 to 0.03374, saving model to .\\model\\lstm_1326_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.03374 to 0.01792, saving model to .\\model\\lstm_1326_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.01792 to 0.01581, saving model to .\\model\\lstm_1326_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.01581 to 0.01074, saving model to .\\model\\lstm_1326_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.01074 to 0.00356, saving model to .\\model\\lstm_1326_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00356 to 0.00248, saving model to .\\model\\lstm_1326_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00248\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00248\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00248\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00248\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00248\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00248\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.00248 to 0.00236, saving model to .\\model\\lstm_1326_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.00236 to 0.00236, saving model to .\\model\\lstm_1326_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.00236 to 0.00235, saving model to .\\model\\lstm_1326_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00235\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00235\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.00235 to 0.00232, saving model to .\\model\\lstm_1326_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00232\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.00232 to 0.00232, saving model to .\\model\\lstm_1326_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00232\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00232\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00232\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00232\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00232\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.00232 to 0.00232, saving model to .\\model\\lstm_1326_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00232\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.00232 to 0.00232, saving model to .\\model\\lstm_1326_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.00232 to 0.00230, saving model to .\\model\\lstm_1326_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00230\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00230\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00230\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00230\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00230\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00230\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.00230 to 0.00230, saving model to .\\model\\lstm_1326_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.00230 to 0.00228, saving model to .\\model\\lstm_1326_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.00228 to 0.00226, saving model to .\\model\\lstm_1326_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00226\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00226\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00226\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00226\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00226\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00226\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00226\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.00226 to 0.00224, saving model to .\\model\\lstm_1326_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00224\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00224\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00224\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.00224 to 0.00224, saving model to .\\model\\lstm_1326_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.00224 to 0.00222, saving model to .\\model\\lstm_1326_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00222\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00222\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.00222 to 0.00220, saving model to .\\model\\lstm_1326_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00220\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00220\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00220\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.00220 to 0.00218, saving model to .\\model\\lstm_1326_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00218\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00218\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00218\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00218\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.00218 to 0.00218, saving model to .\\model\\lstm_1326_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00218\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00218\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00218\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00218\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.00218 to 0.00217, saving model to .\\model\\lstm_1326_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00217\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00217\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00217\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00217\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00217\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00217\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.00217 to 0.00212, saving model to .\\model\\lstm_1326_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.47741, saving model to .\\model\\lstm_1326_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.47741 to 0.33568, saving model to .\\model\\lstm_1326_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.33568 to 0.16727, saving model to .\\model\\lstm_1326_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.16727 to 0.05554, saving model to .\\model\\lstm_1326_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.05554\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.05554\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.05554\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.05554\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.05554\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.05554 to 0.03010, saving model to .\\model\\lstm_1326_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.03010 to 0.01539, saving model to .\\model\\lstm_1326_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.01539 to 0.01338, saving model to .\\model\\lstm_1326_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.01338 to 0.01116, saving model to .\\model\\lstm_1326_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.01116 to 0.00445, saving model to .\\model\\lstm_1326_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00445 to 0.00279, saving model to .\\model\\lstm_1326_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00279\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00279\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00279\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00279\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00279\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00279\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00279\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00279\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00279\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00279\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00279\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00279\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00279\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00279\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00279\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00279\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00279\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00279\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00279\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00279\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00279\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00279\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00279\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00279\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00279\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00279\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00279\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00279\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00279\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00279\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00279\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00279\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00279\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00279\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.00279 to 0.00277, saving model to .\\model\\lstm_1326_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00277\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00277\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00277\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.00277 to 0.00276, saving model to .\\model\\lstm_1326_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00276\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00276\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00276\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00276\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00276\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00276\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.00276 to 0.00275, saving model to .\\model\\lstm_1326_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.00275 to 0.00272, saving model to .\\model\\lstm_1326_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00272\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00272\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00272\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00272\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.00272 to 0.00271, saving model to .\\model\\lstm_1326_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00271\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00271\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00271\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00271\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00271\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.00271 to 0.00269, saving model to .\\model\\lstm_1326_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00269\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00269\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00269\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00269\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00269\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00269\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00269\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.00269 to 0.00267, saving model to .\\model\\lstm_1326_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00267\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00267\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00267\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00267\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.00267 to 0.00266, saving model to .\\model\\lstm_1326_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00266\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00266\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.00266 to 0.00265, saving model to .\\model\\lstm_1326_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00265\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00265\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00265\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00265\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00265\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.00265 to 0.00261, saving model to .\\model\\lstm_1326_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00261\n",
      "5 1326 trading\n",
      "05 1326 最後資產： 19892 交易報酬率： -6.507599999999994 % 持有報酬率 -15.4589 %\n",
      "6 1402 preprocess\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.05485, saving model to .\\model\\lstm_1402_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.05485 to 0.02672, saving model to .\\model\\lstm_1402_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.02672 to 0.00557, saving model to .\\model\\lstm_1402_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.00557\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00557\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00557\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00557\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00557\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00557 to 0.00549, saving model to .\\model\\lstm_1402_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00549\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00549\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00549 to 0.00458, saving model to .\\model\\lstm_1402_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00458 to 0.00284, saving model to .\\model\\lstm_1402_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00284\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04474, saving model to .\\model\\lstm_1402_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04474 to 0.01609, saving model to .\\model\\lstm_1402_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.01609 to 0.00560, saving model to .\\model\\lstm_1402_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.00560\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00560\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00560\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00560\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00560\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00560\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00560\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00560\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00560 to 0.00522, saving model to .\\model\\lstm_1402_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00522 to 0.00430, saving model to .\\model\\lstm_1402_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00430\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00430\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00430\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00430\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00430\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00430\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00430\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00430\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00430\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00430\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00430\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00430\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00430\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00430\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00430\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00430\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00430\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00430\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00430\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00430\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00430\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00430\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00430\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00430\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00430\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00430\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00430\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00430\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00430\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00430\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00430\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00430\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00430\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00430\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00430\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00430\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00430\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00430\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00430\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00430\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00430\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00430\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00430\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00430\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00430\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00430\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00430\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00430\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00430\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.00430 to 0.00430, saving model to .\\model\\lstm_1402_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.00430 to 0.00426, saving model to .\\model\\lstm_1402_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00426\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00426\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.00426 to 0.00425, saving model to .\\model\\lstm_1402_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.00425 to 0.00425, saving model to .\\model\\lstm_1402_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00425\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00425\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00425\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00425\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00425\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00425\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00425\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00425\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00425\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.00425 to 0.00423, saving model to .\\model\\lstm_1402_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00423\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.00423 to 0.00419, saving model to .\\model\\lstm_1402_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.00419 to 0.00417, saving model to .\\model\\lstm_1402_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.00417 to 0.00417, saving model to .\\model\\lstm_1402_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.00417 to 0.00415, saving model to .\\model\\lstm_1402_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.00415 to 0.00411, saving model to .\\model\\lstm_1402_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00411\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00411\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00411\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00411\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.00411 to 0.00410, saving model to .\\model\\lstm_1402_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.00410 to 0.00408, saving model to .\\model\\lstm_1402_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.00408 to 0.00408, saving model to .\\model\\lstm_1402_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.00408 to 0.00407, saving model to .\\model\\lstm_1402_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00407\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.00407 to 0.00407, saving model to .\\model\\lstm_1402_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00407\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00407\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.00407 to 0.00406, saving model to .\\model\\lstm_1402_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.00406 to 0.00401, saving model to .\\model\\lstm_1402_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00401\n",
      "6 1402 trading\n",
      "06 1402 最後資產： 23629 交易報酬率： 11.056300000000007 % 持有報酬率 11.3806 %\n",
      "7 1590 preprocess\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.33823, saving model to .\\model\\lstm_1590_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.33823 to 0.24638, saving model to .\\model\\lstm_1590_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.24638 to 0.14008, saving model to .\\model\\lstm_1590_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.14008 to 0.08531, saving model to .\\model\\lstm_1590_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.08531\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.08531\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.08531\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.08531\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.08531\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.08531 to 0.07167, saving model to .\\model\\lstm_1590_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.07167 to 0.05916, saving model to .\\model\\lstm_1590_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.05916 to 0.05343, saving model to .\\model\\lstm_1590_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.05343 to 0.03667, saving model to .\\model\\lstm_1590_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.03667 to 0.01505, saving model to .\\model\\lstm_1590_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.01505 to 0.01048, saving model to .\\model\\lstm_1590_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.01048\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.32464, saving model to .\\model\\lstm_1590_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.32464 to 0.23311, saving model to .\\model\\lstm_1590_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.23311 to 0.12045, saving model to .\\model\\lstm_1590_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.12045 to 0.05925, saving model to .\\model\\lstm_1590_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.05925\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.05925\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.05925\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.05925\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.05925\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.05925 to 0.05476, saving model to .\\model\\lstm_1590_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.05476 to 0.04924, saving model to .\\model\\lstm_1590_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.04924 to 0.04779, saving model to .\\model\\lstm_1590_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.04779 to 0.03436, saving model to .\\model\\lstm_1590_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.03436 to 0.01861, saving model to .\\model\\lstm_1590_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.01861 to 0.01516, saving model to .\\model\\lstm_1590_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.01516\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.01516\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.01516\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.01516\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.01516\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.01516\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.01516\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.01516\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.01516\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.01516\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.01516\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.01516\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.01516\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.01516\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.01516\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.01516\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.01516\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.01516\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.01516\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.01516\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.01516\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.01516\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.01516\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.01516\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.01516\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.01516\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.01516\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.01516\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.01516\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.01516\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.01516\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.01516\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.01516\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.01516\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.01516\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.01516\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.01516\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.01516\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.01516\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.01516\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.01516\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.01516\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.01516\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.01516\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.01516\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.01516\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.01516\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.01516\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.01516\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.01516\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.01516\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.01516\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.01516\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.01516\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.01516\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.01516\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.01516\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.01516\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.01516\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.01516\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.01516\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.01516\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.01516\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.01516\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.01516\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.01516\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.01516\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.01516\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.01516\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.01516\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.01516\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.01516\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.01516\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.01516\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.01516\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.01516\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.01516\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.01516\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.01516\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.01516\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.01516\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.01516\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.01516\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.01516\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.01516\n",
      "7 1590 trading\n",
      "07 1590 最後資產： 20880 交易報酬率： -1.8639999999999939 % 持有報酬率 -10.8031 %\n",
      "8 2002 preprocess\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.45720, saving model to .\\model\\lstm_2002_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.45720 to 0.30823, saving model to .\\model\\lstm_2002_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.30823 to 0.12878, saving model to .\\model\\lstm_2002_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.12878 to 0.00384, saving model to .\\model\\lstm_2002_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00384\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00384\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00384\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00384\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00384\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00384\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00384 to 0.00241, saving model to .\\model\\lstm_2002_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.00241\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00241\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.00241 to 0.00204, saving model to .\\model\\lstm_2002_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00204\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00204\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00204\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00204\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.00204 to 0.00177, saving model to .\\model\\lstm_2002_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.00177 to 0.00177, saving model to .\\model\\lstm_2002_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00177\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00177\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00177\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00177\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00177\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00177\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00177\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00177\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00177\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00177\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00177\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00177\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00177\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00177\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00177\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00177\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00177\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00177\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00177\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00177\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00177\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00177\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00177\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00177\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00177\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00177\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00177\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00177\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00177\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00177\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00177\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00177\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00177\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00177\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00177\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00177\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00177\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00177\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00177\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00177\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00177\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00177\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00177\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00177\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00177\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00177\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00177\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00177\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00177\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00177\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00177\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00177\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00177\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00177\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00177\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00177\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00177\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00177\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00177\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00177\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00177\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00177\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00177\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00177\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00177\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00177\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00177\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00177\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00177\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00177\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00177\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00177\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00177\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00177\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00177\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00177\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00177\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00177\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00177\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00177\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.52325, saving model to .\\model\\lstm_2002_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.52325 to 0.40701, saving model to .\\model\\lstm_2002_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.40701 to 0.24917, saving model to .\\model\\lstm_2002_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.24917 to 0.07313, saving model to .\\model\\lstm_2002_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.07313 to 0.00334, saving model to .\\model\\lstm_2002_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00334\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00334\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00334 to 0.00298, saving model to .\\model\\lstm_2002_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00298\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00298\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00298\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00298 to 0.00215, saving model to .\\model\\lstm_2002_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00215\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00215\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00215\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00215\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00215\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00215\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00215\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00215\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00215\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00215\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00215\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00215\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00215\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00215\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.00215 to 0.00210, saving model to .\\model\\lstm_2002_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00210\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00210\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00210\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00210\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00210\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00210\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00210\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00210\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00210\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00210\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00210\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00210\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00210\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00210\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00210\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00210\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00210\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00210\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00210\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00210\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00210\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00210\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00210\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00210\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00210\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00210\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00210\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00210\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00210\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00210\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00210\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00210\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00210\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00210\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00210\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00210\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00210\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00210\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00210\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00210\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00210\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00210\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00210\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00210\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00210\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00210\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00210\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00210\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00210\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00210\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00210\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00210\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00210\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00210\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00210\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00210\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00210\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00210\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00210\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00210\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00210\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00210\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00210\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00210\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00210\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00210\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00210\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00210\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00210\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00210\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00210\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00210\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00210\n",
      "8 2002 trading\n",
      "08 2002 最後資產： 19878 交易報酬率： -6.573399999999993 % 持有報酬率 -3.2389 %\n",
      "9 2207 preprocess\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.14265, saving model to .\\model\\lstm_2207_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.14265 to 0.08373, saving model to .\\model\\lstm_2207_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.08373 to 0.02339, saving model to .\\model\\lstm_2207_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.02339 to 0.00392, saving model to .\\model\\lstm_2207_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00392\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00392\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00392\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00392 to 0.00140, saving model to .\\model\\lstm_2207_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.13344, saving model to .\\model\\lstm_2207_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.13344 to 0.07161, saving model to .\\model\\lstm_2207_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.07161 to 0.01594, saving model to .\\model\\lstm_2207_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.01594 to 0.00743, saving model to .\\model\\lstm_2207_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00743\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00743\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00743 to 0.00249, saving model to .\\model\\lstm_2207_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00249 to 0.00124, saving model to .\\model\\lstm_2207_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00124 to 0.00122, saving model to .\\model\\lstm_2207_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00122\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00122\n",
      "9 2207 trading\n",
      "09 2207 最後資產： 25934 交易報酬率： 21.889800000000008 % 持有報酬率 90.2507 %\n",
      "10 2303 preprocess\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.29225, saving model to .\\model\\lstm_2303_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.29225 to 0.19821, saving model to .\\model\\lstm_2303_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.19821 to 0.08504, saving model to .\\model\\lstm_2303_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.08504 to 0.02521, saving model to .\\model\\lstm_2303_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.02521\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.02521\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.02521\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.02521\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.02521\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.02521 to 0.02469, saving model to .\\model\\lstm_2303_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.02469 to 0.01870, saving model to .\\model\\lstm_2303_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.01870\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.01870 to 0.01835, saving model to .\\model\\lstm_2303_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.01835 to 0.01227, saving model to .\\model\\lstm_2303_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.01227 to 0.00779, saving model to .\\model\\lstm_2303_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.00779 to 0.00736, saving model to .\\model\\lstm_2303_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00736\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00736\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00736\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00736\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.00736 to 0.00728, saving model to .\\model\\lstm_2303_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.00728 to 0.00725, saving model to .\\model\\lstm_2303_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.00725 to 0.00714, saving model to .\\model\\lstm_2303_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.00714 to 0.00710, saving model to .\\model\\lstm_2303_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00710\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00710\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00710\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.00710 to 0.00697, saving model to .\\model\\lstm_2303_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.00697 to 0.00694, saving model to .\\model\\lstm_2303_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00694\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00694\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.00694 to 0.00693, saving model to .\\model\\lstm_2303_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.00693 to 0.00687, saving model to .\\model\\lstm_2303_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00687\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.00687 to 0.00678, saving model to .\\model\\lstm_2303_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00678\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00678\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.00678 to 0.00675, saving model to .\\model\\lstm_2303_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00675\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.00675 to 0.00673, saving model to .\\model\\lstm_2303_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00673\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00673\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00673\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.00673 to 0.00669, saving model to .\\model\\lstm_2303_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.00669 to 0.00664, saving model to .\\model\\lstm_2303_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.00664 to 0.00662, saving model to .\\model\\lstm_2303_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00662\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00662\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.00662 to 0.00661, saving model to .\\model\\lstm_2303_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00661\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00661\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.00661 to 0.00657, saving model to .\\model\\lstm_2303_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.00657 to 0.00653, saving model to .\\model\\lstm_2303_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00653\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00653\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00653\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00653\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.00653 to 0.00644, saving model to .\\model\\lstm_2303_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00644\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00644\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00644\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00644\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00644\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00644\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.00644 to 0.00637, saving model to .\\model\\lstm_2303_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00637\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00637\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00637\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00637\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00637\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00637\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00637\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00637\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.00637 to 0.00635, saving model to .\\model\\lstm_2303_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00635\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.00635 to 0.00624, saving model to .\\model\\lstm_2303_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00624\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00624\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00624\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00624\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00624\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.00624 to 0.00617, saving model to .\\model\\lstm_2303_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.00617 to 0.00615, saving model to .\\model\\lstm_2303_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00615\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00615\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00615\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00615\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00615\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00615\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00615\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00615\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00615\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00615\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.00615 to 0.00614, saving model to .\\model\\lstm_2303_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00614\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.00614 to 0.00608, saving model to .\\model\\lstm_2303_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00608\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.00608 to 0.00599, saving model to .\\model\\lstm_2303_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00599\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.29444, saving model to .\\model\\lstm_2303_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.29444 to 0.19492, saving model to .\\model\\lstm_2303_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.19492 to 0.08453, saving model to .\\model\\lstm_2303_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.08453 to 0.02281, saving model to .\\model\\lstm_2303_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.02281 to 0.02195, saving model to .\\model\\lstm_2303_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.02195\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.02195\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.02195\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.02195\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.02195 to 0.02047, saving model to .\\model\\lstm_2303_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.02047 to 0.01506, saving model to .\\model\\lstm_2303_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.01506 to 0.01498, saving model to .\\model\\lstm_2303_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.01498\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.01498 to 0.01317, saving model to .\\model\\lstm_2303_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.01317 to 0.00983, saving model to .\\model\\lstm_2303_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.00983 to 0.00913, saving model to .\\model\\lstm_2303_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00913\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00913\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00913\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00913\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00913\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00913\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00913\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00913\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00913\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00913\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00913\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00913\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00913\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00913\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00913\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00913\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.00913 to 0.00889, saving model to .\\model\\lstm_2303_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00889\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00889\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00889\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00889\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00889\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00889\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00889\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00889\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00889\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00889\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00889\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00889\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00889\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.00889 to 0.00885, saving model to .\\model\\lstm_2303_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00885\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.00885 to 0.00883, saving model to .\\model\\lstm_2303_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00883\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.00883 to 0.00866, saving model to .\\model\\lstm_2303_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00866\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00866\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00866\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00866\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.00866 to 0.00861, saving model to .\\model\\lstm_2303_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00861\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00861\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00861\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00861\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.00861 to 0.00859, saving model to .\\model\\lstm_2303_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.00859 to 0.00853, saving model to .\\model\\lstm_2303_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00853\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.00853 to 0.00851, saving model to .\\model\\lstm_2303_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00851\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.00851 to 0.00832, saving model to .\\model\\lstm_2303_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00832\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00832\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.00832 to 0.00818, saving model to .\\model\\lstm_2303_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00818\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00818\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.00818 to 0.00817, saving model to .\\model\\lstm_2303_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.00817 to 0.00816, saving model to .\\model\\lstm_2303_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00816\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.00816 to 0.00812, saving model to .\\model\\lstm_2303_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00812\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00812\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00812\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.00812 to 0.00803, saving model to .\\model\\lstm_2303_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.00803 to 0.00796, saving model to .\\model\\lstm_2303_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00796\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00796\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.00796 to 0.00783, saving model to .\\model\\lstm_2303_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.00783 to 0.00773, saving model to .\\model\\lstm_2303_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00773\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00773\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00773\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00773\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00773\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00773\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00773\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00773\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00773\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00773\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00773\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00773\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00773\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00773\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00773\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00773\n",
      "10 2303 trading\n",
      "10 2303 最後資產： 19912 交易報酬率： -6.4135999999999935 % 持有報酬率 15.4386 %\n",
      "11 2308 preprocess\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.20654, saving model to .\\model\\lstm_2308_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.20654 to 0.14028, saving model to .\\model\\lstm_2308_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.14028 to 0.06459, saving model to .\\model\\lstm_2308_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.06459 to 0.01150, saving model to .\\model\\lstm_2308_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.01150\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.01150\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.01150\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.01150 to 0.00795, saving model to .\\model\\lstm_2308_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00795\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00795 to 0.00642, saving model to .\\model\\lstm_2308_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00642\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.00642\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00642 to 0.00538, saving model to .\\model\\lstm_2308_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.00538 to 0.00220, saving model to .\\model\\lstm_2308_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00220 to 0.00205, saving model to .\\model\\lstm_2308_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.00205 to 0.00125, saving model to .\\model\\lstm_2308_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.00125 to 0.00124, saving model to .\\model\\lstm_2308_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.19428, saving model to .\\model\\lstm_2308_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.19428 to 0.13163, saving model to .\\model\\lstm_2308_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.13163 to 0.05689, saving model to .\\model\\lstm_2308_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.05689 to 0.01166, saving model to .\\model\\lstm_2308_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.01166\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.01166\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.01166\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.01166 to 0.01122, saving model to .\\model\\lstm_2308_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.01122 to 0.00963, saving model to .\\model\\lstm_2308_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00963 to 0.00909, saving model to .\\model\\lstm_2308_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00909\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.00909\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00909\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.00909 to 0.00761, saving model to .\\model\\lstm_2308_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00761 to 0.00424, saving model to .\\model\\lstm_2308_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.00424 to 0.00332, saving model to .\\model\\lstm_2308_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.00332 to 0.00222, saving model to .\\model\\lstm_2308_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.00222 to 0.00164, saving model to .\\model\\lstm_2308_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.00164 to 0.00149, saving model to .\\model\\lstm_2308_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00149\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00149\n",
      "11 2308 trading\n",
      "11 2308 最後資產： 24594 交易報酬率： 15.591800000000008 % 持有報酬率 7.4468 %\n",
      "12 2317 preprocess\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.47419, saving model to .\\model\\lstm_2317_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.47419 to 0.35610, saving model to .\\model\\lstm_2317_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.35610 to 0.19747, saving model to .\\model\\lstm_2317_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.19747 to 0.05302, saving model to .\\model\\lstm_2317_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.05302 to 0.04310, saving model to .\\model\\lstm_2317_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.04310\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.04310\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.04310\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.04310\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.04310\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.04310 to 0.03692, saving model to .\\model\\lstm_2317_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.03692\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.03692\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.03692\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.03692 to 0.03665, saving model to .\\model\\lstm_2317_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.03665 to 0.03501, saving model to .\\model\\lstm_2317_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.03501\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.03501\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.03501\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.03501\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.03501\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.03501\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.03501\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.03501\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.03501\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.03501\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.03501\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.03501\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.03501\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.03501\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.03501\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.03501\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.03501\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.03501\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.03501\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.03501\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.03501\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.03501\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.03501\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.03501\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.03501\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.03501\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.03501\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.03501\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.03501\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.03501\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.03501\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.03501\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.03501\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.03501\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.03501\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.03501\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.03501\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.03501\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.03501\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.03501\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.03501\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.03501\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.03501\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.03501\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.03501\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.03501\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.03501\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.03501\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.03501\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.03501\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.03501\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.03501\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.03501\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.03501\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.03501\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.03501\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.03501\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.03501\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.03501\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.03501\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.03501\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.03501\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.03501\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.03501\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.03501\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.03501\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.03501\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.03501\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.03501\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.03501\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.03501\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.03501\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.03501\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.03501\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.03501\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.03501\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.03501\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.03501\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.03501\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.03501\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.03501\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.03501\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.03501\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.03501\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.45626, saving model to .\\model\\lstm_2317_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.45626 to 0.30858, saving model to .\\model\\lstm_2317_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.30858 to 0.13968, saving model to .\\model\\lstm_2317_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.13968 to 0.03560, saving model to .\\model\\lstm_2317_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.03560\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.03560\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.03560\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.03560\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.03560\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.03560 to 0.03426, saving model to .\\model\\lstm_2317_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.03426 to 0.02959, saving model to .\\model\\lstm_2317_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.02959\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.02959\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.02959\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.02959\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.02959\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.02959\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.02959\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.02959\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.02959\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.02959\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.02959\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.02959\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.02959\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.02959\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.02959\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.02959\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.02959\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.02959 to 0.02951, saving model to .\\model\\lstm_2317_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.02951\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.02951\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.02951\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.02951\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.02951\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.02951\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.02951\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.02951\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.02951\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.02951\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.02951\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.02951\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.02951\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.02951\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.02951\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.02951\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.02951\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.02951\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.02951\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.02951\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.02951\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.02951\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.02951\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.02951\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.02951\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.02951\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.02951\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.02951\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.02951\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.02951\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.02951\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.02951\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.02951\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.02951\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.02951\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.02951\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.02951\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.02951\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.02951\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.02951\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.02951\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.02951\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.02951\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.02951\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.02951\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.02951\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.02951\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.02951\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.02951\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.02951\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.02951\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.02951\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.02951\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.02951\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.02951\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.02951\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.02951\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.02951\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.02951\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.02951\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.02951\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.02951\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.02951\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.02951\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.02951\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.02951\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.02951\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.02951\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.02951\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.02951\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.02951\n",
      "12 2317 trading\n",
      "12 2317 最後資產： 19282 交易報酬率： -9.374599999999994 % 持有報酬率 -23.5368 %\n",
      "13 2324 preprocess\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.09647, saving model to .\\model\\lstm_2324_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.09647 to 0.05387, saving model to .\\model\\lstm_2324_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.05387 to 0.01193, saving model to .\\model\\lstm_2324_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.01193 to 0.01150, saving model to .\\model\\lstm_2324_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.01150\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.01150 to 0.00371, saving model to .\\model\\lstm_2324_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00371 to 0.00268, saving model to .\\model\\lstm_2324_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00268\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00268 to 0.00183, saving model to .\\model\\lstm_2324_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00183\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00183\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.00183\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00183 to 0.00125, saving model to .\\model\\lstm_2324_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.00125 to 0.00114, saving model to .\\model\\lstm_2324_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00114\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00114\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00114\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00114\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00114\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00114\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00114\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00114\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.00114 to 0.00112, saving model to .\\model\\lstm_2324_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00112\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.00112 to 0.00111, saving model to .\\model\\lstm_2324_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00111\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.00111 to 0.00109, saving model to .\\model\\lstm_2324_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00109\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00109\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.00109 to 0.00108, saving model to .\\model\\lstm_2324_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00108\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00108\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00108\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00108\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.00108 to 0.00107, saving model to .\\model\\lstm_2324_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00107\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.00107 to 0.00107, saving model to .\\model\\lstm_2324_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00107\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00107\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00107\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.00107 to 0.00107, saving model to .\\model\\lstm_2324_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00107\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00107\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00107\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00107\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00107\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00107\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00107\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00107\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00107\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00107\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00107\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00107\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00107\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00107\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00107\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00107\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00107\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00107\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00107\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00107\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00107\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00107\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.00107 to 0.00107, saving model to .\\model\\lstm_2324_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00107\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00107\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00107\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00107\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00107\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00107\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00107\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00107\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00107\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00107\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00107\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00107\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00107\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00107\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00107\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00107\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00107\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00107\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00107\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00107\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00107\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00107\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00107\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00107\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00107\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00107\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00107\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00107\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00107\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00107\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00107\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00107\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00107\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00107\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00107\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00107\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.09727, saving model to .\\model\\lstm_2324_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.09727 to 0.05760, saving model to .\\model\\lstm_2324_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.05760 to 0.01771, saving model to .\\model\\lstm_2324_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.01771 to 0.00572, saving model to .\\model\\lstm_2324_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00572\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00572\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00572 to 0.00261, saving model to .\\model\\lstm_2324_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00261\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00261 to 0.00229, saving model to .\\model\\lstm_2324_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00229\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00229\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.00229\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00229 to 0.00223, saving model to .\\model\\lstm_2324_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.00223 to 0.00137, saving model to .\\model\\lstm_2324_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00137\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00137\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.00137 to 0.00137, saving model to .\\model\\lstm_2324_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00137\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00137\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00137\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.00137 to 0.00130, saving model to .\\model\\lstm_2324_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.00130 to 0.00125, saving model to .\\model\\lstm_2324_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00125\n",
      "13 2324 trading\n",
      "13 2324 最後資產： 19831 交易報酬率： -6.794299999999994 % 持有報酬率 -10.6635 %\n",
      "14 2327 preprocess\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.20817, saving model to .\\model\\lstm_2327_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.20817 to 0.15833, saving model to .\\model\\lstm_2327_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.15833 to 0.11598, saving model to .\\model\\lstm_2327_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.11598 to 0.10058, saving model to .\\model\\lstm_2327_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.10058 to 0.09597, saving model to .\\model\\lstm_2327_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.09597 to 0.07974, saving model to .\\model\\lstm_2327_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.07974 to 0.05224, saving model to .\\model\\lstm_2327_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.05224 to 0.03194, saving model to .\\model\\lstm_2327_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.03194\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.20454, saving model to .\\model\\lstm_2327_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.20454 to 0.14687, saving model to .\\model\\lstm_2327_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.14687 to 0.10772, saving model to .\\model\\lstm_2327_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.10772 to 0.09471, saving model to .\\model\\lstm_2327_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.09471 to 0.08388, saving model to .\\model\\lstm_2327_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.08388 to 0.06056, saving model to .\\model\\lstm_2327_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.06056 to 0.03519, saving model to .\\model\\lstm_2327_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.03519 to 0.03084, saving model to .\\model\\lstm_2327_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.03084\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.03084\n",
      "14 2327 trading\n",
      "14 2327 最後資產： 21276 交易報酬率： -0.0027999999999938157 % 持有報酬率 47.9031 %\n",
      "15 2330 preprocess\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.52262, saving model to .\\model\\lstm_2330_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.52262 to 0.41542, saving model to .\\model\\lstm_2330_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.41542 to 0.26938, saving model to .\\model\\lstm_2330_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.26938 to 0.12412, saving model to .\\model\\lstm_2330_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.12412 to 0.10942, saving model to .\\model\\lstm_2330_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.10942\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.10942\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.10942 to 0.09645, saving model to .\\model\\lstm_2330_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.09645 to 0.03324, saving model to .\\model\\lstm_2330_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.03324 to 0.00967, saving model to .\\model\\lstm_2330_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00967 to 0.00939, saving model to .\\model\\lstm_2330_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00939 to 0.00703, saving model to .\\model\\lstm_2330_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00703 to 0.00487, saving model to .\\model\\lstm_2330_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00487\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.49489, saving model to .\\model\\lstm_2330_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.49489 to 0.36096, saving model to .\\model\\lstm_2330_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36096 to 0.20537, saving model to .\\model\\lstm_2330_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.20537 to 0.12085, saving model to .\\model\\lstm_2330_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.12085\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.12085\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.12085\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.12085\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.12085 to 0.06593, saving model to .\\model\\lstm_2330_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.06593 to 0.03163, saving model to .\\model\\lstm_2330_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.03163 to 0.02065, saving model to .\\model\\lstm_2330_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.02065 to 0.01085, saving model to .\\model\\lstm_2330_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.01085 to 0.00405, saving model to .\\model\\lstm_2330_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00405\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00405\n",
      "15 2330 trading\n",
      "15 2330 最後資產： 21276 交易報酬率： -0.0027999999999938157 % 持有報酬率 42.3656 %\n",
      "16 2357 preprocess\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.09728, saving model to .\\model\\lstm_2357_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.09728 to 0.04168, saving model to .\\model\\lstm_2357_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04168 to 0.00559, saving model to .\\model\\lstm_2357_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.00559\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00559\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00559\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00559\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00559\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00559\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00559\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00559\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.00559\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00559\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00559\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00559\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00559\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00559\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00559\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00559\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00559\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00559\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.00559 to 0.00454, saving model to .\\model\\lstm_2357_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.00454 to 0.00352, saving model to .\\model\\lstm_2357_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00352\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00352\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.00352 to 0.00315, saving model to .\\model\\lstm_2357_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00315\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00315\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00315\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00315\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00315\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00315\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00315\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00315\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00315\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00315\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00315\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00315\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00315\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00315\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00315\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00315\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00315\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00315\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.00315 to 0.00313, saving model to .\\model\\lstm_2357_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00313\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00313\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.00313 to 0.00308, saving model to .\\model\\lstm_2357_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00308\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00308\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00308\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00308\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00308\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00308\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00308\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00308\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00308\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00308\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00308\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00308\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00308\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00308\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00308\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00308\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00308\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00308\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00308\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00308\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00308\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00308\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00308\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00308\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00308\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00308\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00308\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00308\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00308\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00308\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00308\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00308\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00308\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00308\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00308\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00308\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00308\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00308\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00308\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00308\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00308\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00308\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00308\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00308\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00308\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00308\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00308\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00308\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00308\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00308\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00308\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00308\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.10955, saving model to .\\model\\lstm_2357_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.10955 to 0.06351, saving model to .\\model\\lstm_2357_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.06351 to 0.01880, saving model to .\\model\\lstm_2357_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.01880 to 0.01291, saving model to .\\model\\lstm_2357_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.01291\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.01291\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.01291 to 0.01114, saving model to .\\model\\lstm_2357_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.01114 to 0.00658, saving model to .\\model\\lstm_2357_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00658\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00658\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00658\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.00658\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00658\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00658\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00658\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00658\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00658\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00658\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00658\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00658\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00658\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.00658 to 0.00539, saving model to .\\model\\lstm_2357_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.00539 to 0.00445, saving model to .\\model\\lstm_2357_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00445\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.00445 to 0.00402, saving model to .\\model\\lstm_2357_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00402\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.00402 to 0.00385, saving model to .\\model\\lstm_2357_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.00385 to 0.00380, saving model to .\\model\\lstm_2357_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00380\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00380\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00380\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00380\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00380\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00380\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00380\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00380\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.00380 to 0.00376, saving model to .\\model\\lstm_2357_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.00376 to 0.00363, saving model to .\\model\\lstm_2357_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00363\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00363\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.00363 to 0.00348, saving model to .\\model\\lstm_2357_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00348\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00348\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00348\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.00348 to 0.00348, saving model to .\\model\\lstm_2357_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00348\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00348\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00348\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00348\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00348\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00348\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00348\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00348\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.00348 to 0.00336, saving model to .\\model\\lstm_2357_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00336\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00336\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00336\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00336\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00336\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00336\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00336\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00336\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00336\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.00336 to 0.00333, saving model to .\\model\\lstm_2357_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00333\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00333\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00333\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00333\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00333\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00333\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00333\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00333\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00333\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00333\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00333\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00333\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00333\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00333\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00333\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00333\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00333\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00333\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00333\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00333\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00333\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00333\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00333\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00333\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00333\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00333\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00333\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00333\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00333\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00333\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00333\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00333\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00333\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00333\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00333\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00333\n",
      "16 2357 trading\n",
      "16 2357 最後資產： 20542 交易報酬率： -3.452599999999994 % 持有報酬率 -16.7266 %\n",
      "17 2379 preprocess\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.46354, saving model to .\\model\\lstm_2379_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.46354 to 0.35372, saving model to .\\model\\lstm_2379_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.35372 to 0.20439, saving model to .\\model\\lstm_2379_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.20439 to 0.05043, saving model to .\\model\\lstm_2379_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.05043 to 0.00438, saving model to .\\model\\lstm_2379_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00438\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00438\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00438\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00438\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00438\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00438 to 0.00346, saving model to .\\model\\lstm_2379_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00346 to 0.00222, saving model to .\\model\\lstm_2379_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00222 to 0.00218, saving model to .\\model\\lstm_2379_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.00218 to 0.00214, saving model to .\\model\\lstm_2379_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00214\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00214\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00214\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00214\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00214\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00214\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00214\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00214\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00214\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00214\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00214\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00214\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00214\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00214\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00214\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00214\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00214\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00214\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00214\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00214\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00214\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00214\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00214\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00214\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00214\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00214\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00214\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00214\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00214\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00214\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00214\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00214\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.00214 to 0.00212, saving model to .\\model\\lstm_2379_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.00212 to 0.00208, saving model to .\\model\\lstm_2379_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.00208 to 0.00206, saving model to .\\model\\lstm_2379_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00206\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00206\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00206\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00206\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00206\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00206\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00206\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00206\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00206\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00206\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00206\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00206\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00206\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00206\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00206\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00206\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00206\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00206\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00206\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00206\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00206\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00206\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00206\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00206\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00206\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00206\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00206\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00206\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00206\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00206\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00206\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.00206 to 0.00204, saving model to .\\model\\lstm_2379_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00204\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00204\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00204\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.00204 to 0.00197, saving model to .\\model\\lstm_2379_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00197\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.43669, saving model to .\\model\\lstm_2379_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.43669 to 0.30848, saving model to .\\model\\lstm_2379_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.30848 to 0.15992, saving model to .\\model\\lstm_2379_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.15992 to 0.03489, saving model to .\\model\\lstm_2379_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.03489 to 0.00891, saving model to .\\model\\lstm_2379_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00891\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00891\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00891\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00891\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00891\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00891 to 0.00378, saving model to .\\model\\lstm_2379_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00378 to 0.00267, saving model to .\\model\\lstm_2379_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00267\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.00267 to 0.00245, saving model to .\\model\\lstm_2379_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00245\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00245\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00245\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00245\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00245\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00245\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00245\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00245\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00245\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00245\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00245\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00245\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00245\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00245\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00245\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00245\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00245\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00245\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00245\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00245\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.00245 to 0.00245, saving model to .\\model\\lstm_2379_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00245\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00245\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00245\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.00245 to 0.00243, saving model to .\\model\\lstm_2379_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00243\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00243\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.00243 to 0.00243, saving model to .\\model\\lstm_2379_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00243\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.00243 to 0.00240, saving model to .\\model\\lstm_2379_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.00240 to 0.00239, saving model to .\\model\\lstm_2379_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00239\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00239\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00239\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00239\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00239\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00239\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.00239 to 0.00238, saving model to .\\model\\lstm_2379_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00238\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00238\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.00238 to 0.00238, saving model to .\\model\\lstm_2379_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00238\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00238\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.00238 to 0.00237, saving model to .\\model\\lstm_2379_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00237\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00237\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00237\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00237\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.00237 to 0.00236, saving model to .\\model\\lstm_2379_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.00236 to 0.00234, saving model to .\\model\\lstm_2379_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00234\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00234\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00234\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00234\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.00234 to 0.00231, saving model to .\\model\\lstm_2379_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00231\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00231\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00231\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00231\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00231\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00231\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.00231 to 0.00229, saving model to .\\model\\lstm_2379_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00229\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00229\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00229\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00229\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00229\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00229\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00229\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00229\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00229\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00229\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.00229 to 0.00229, saving model to .\\model\\lstm_2379_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00229\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00229\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00229\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00229\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00229\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00229\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00229\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00229\n",
      "17 2379 trading\n",
      "17 2379 最後資產： 24779 交易報酬率： 16.46130000000001 % 持有報酬率 115.5963 %\n",
      "18 2382 preprocess\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.16752, saving model to .\\model\\lstm_2382_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.16752 to 0.09350, saving model to .\\model\\lstm_2382_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.09350 to 0.01863, saving model to .\\model\\lstm_2382_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.01863\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.01863\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.01863 to 0.00666, saving model to .\\model\\lstm_2382_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00666\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00666\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00666 to 0.00593, saving model to .\\model\\lstm_2382_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00593\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00593\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00593 to 0.00544, saving model to .\\model\\lstm_2382_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00544 to 0.00396, saving model to .\\model\\lstm_2382_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00396\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00396\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00396\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.00396 to 0.00394, saving model to .\\model\\lstm_2382_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00394\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00394\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.00394 to 0.00353, saving model to .\\model\\lstm_2382_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00353\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00353\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00353\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00353\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00353\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00353\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00353\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00353\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00353\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00353\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00353\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00353\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00353\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00353\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00353\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00353\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00353\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00353\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00353\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00353\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00353\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00353\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00353\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00353\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00353\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00353\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00353\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00353\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00353\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00353\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00353\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00353\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00353\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00353\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00353\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00353\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00353\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00353\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00353\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00353\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00353\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00353\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00353\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00353\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00353\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00353\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00353\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00353\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00353\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00353\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00353\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00353\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00353\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00353\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00353\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00353\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00353\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00353\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00353\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00353\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00353\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00353\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00353\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00353\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00353\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00353\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00353\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00353\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00353\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00353\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00353\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00353\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00353\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00353\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00353\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00353\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00353\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00353\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00353\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00353\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.18467, saving model to .\\model\\lstm_2382_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.18467 to 0.11794, saving model to .\\model\\lstm_2382_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.11794 to 0.04531, saving model to .\\model\\lstm_2382_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.04531 to 0.00963, saving model to .\\model\\lstm_2382_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00963\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00963\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00963 to 0.00825, saving model to .\\model\\lstm_2382_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00825\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00825 to 0.00681, saving model to .\\model\\lstm_2382_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00681\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00681\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.00681\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00681 to 0.00448, saving model to .\\model\\lstm_2382_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.00448 to 0.00421, saving model to .\\model\\lstm_2382_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00421\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00421\n",
      "18 2382 trading\n",
      "18 2382 最後資產： 19883 交易報酬率： -6.549899999999994 % 持有報酬率 3.8772 %\n",
      "19 2395 preprocess\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.46534, saving model to .\\model\\lstm_2395_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.46534 to 0.34329, saving model to .\\model\\lstm_2395_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.34329 to 0.17630, saving model to .\\model\\lstm_2395_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.17630 to 0.01915, saving model to .\\model\\lstm_2395_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.01915\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.01915 to 0.00828, saving model to .\\model\\lstm_2395_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00828\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00828\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00828\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00828\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00828 to 0.00664, saving model to .\\model\\lstm_2395_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00664 to 0.00552, saving model to .\\model\\lstm_2395_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00552\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00552\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00552\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00552\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.00552 to 0.00541, saving model to .\\model\\lstm_2395_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00541\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00541\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00541\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00541\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00541\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00541\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00541\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00541\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00541\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00541\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00541\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00541\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00541\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00541\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00541\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00541\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00541\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.00541 to 0.00533, saving model to .\\model\\lstm_2395_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00533\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00533\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.00533 to 0.00524, saving model to .\\model\\lstm_2395_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.00524 to 0.00524, saving model to .\\model\\lstm_2395_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.00524 to 0.00515, saving model to .\\model\\lstm_2395_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.00515 to 0.00505, saving model to .\\model\\lstm_2395_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.00505 to 0.00476, saving model to .\\model\\lstm_2395_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00476\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00476\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00476\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00476\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00476\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00476\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00476\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00476\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.00476 to 0.00468, saving model to .\\model\\lstm_2395_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00468\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00468\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00468\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00468\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00468\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00468\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00468\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00468\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00468\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00468\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00468\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00468\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00468\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00468\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00468\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.00468 to 0.00461, saving model to .\\model\\lstm_2395_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00461\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00461\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00461\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00461\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00461\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00461\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00461\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.00461 to 0.00456, saving model to .\\model\\lstm_2395_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00456\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00456\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00456\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00456\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00456\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00456\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00456\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.00456 to 0.00447, saving model to .\\model\\lstm_2395_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.00447 to 0.00406, saving model to .\\model\\lstm_2395_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00406\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00406\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00406\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00406\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.00406 to 0.00382, saving model to .\\model\\lstm_2395_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00382\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00382\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.00382 to 0.00381, saving model to .\\model\\lstm_2395_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00381\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00381\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00381\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00381\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00381\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.00381 to 0.00378, saving model to .\\model\\lstm_2395_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00378\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.00378 to 0.00323, saving model to .\\model\\lstm_2395_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.46734, saving model to .\\model\\lstm_2395_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.46734 to 0.34528, saving model to .\\model\\lstm_2395_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.34528 to 0.19581, saving model to .\\model\\lstm_2395_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.19581 to 0.05788, saving model to .\\model\\lstm_2395_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.05788 to 0.00607, saving model to .\\model\\lstm_2395_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00607\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00607\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00607\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00607\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00607\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00607\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.00607\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00607\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00607\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00607\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00607\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00607\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00607\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00607\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.00607 to 0.00587, saving model to .\\model\\lstm_2395_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.00587 to 0.00578, saving model to .\\model\\lstm_2395_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.00578 to 0.00564, saving model to .\\model\\lstm_2395_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00564\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00564\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00564\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00564\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00564\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00564\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00564\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00564\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00564\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00564\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00564\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00564\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00564\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00564\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.00564 to 0.00544, saving model to .\\model\\lstm_2395_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.00544 to 0.00536, saving model to .\\model\\lstm_2395_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00536\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00536\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00536\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00536\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00536\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00536\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00536\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00536\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00536\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00536\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.00536 to 0.00534, saving model to .\\model\\lstm_2395_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00534\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00534\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00534\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00534\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.00534 to 0.00489, saving model to .\\model\\lstm_2395_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00489\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00489\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00489\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00489\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00489\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00489\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00489\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00489\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00489\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00489\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00489\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00489\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00489\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00489\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00489\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00489\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00489\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00489\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00489\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00489\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00489\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.00489 to 0.00471, saving model to .\\model\\lstm_2395_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.00471 to 0.00461, saving model to .\\model\\lstm_2395_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00461\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00461\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00461\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00461\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00461\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00461\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00461\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00461\n",
      "19 2395 trading\n",
      "19 2395 最後資產： 23365 交易報酬率： 9.815500000000007 % 持有報酬率 40.4651 %\n",
      "20 2408 preprocess\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.27369, saving model to .\\model\\lstm_2408_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.27369 to 0.17429, saving model to .\\model\\lstm_2408_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.17429 to 0.06446, saving model to .\\model\\lstm_2408_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.06446 to 0.02478, saving model to .\\model\\lstm_2408_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.02478 to 0.02078, saving model to .\\model\\lstm_2408_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.02078\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.02078\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.02078\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.02078 to 0.01164, saving model to .\\model\\lstm_2408_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.01164 to 0.00880, saving model to .\\model\\lstm_2408_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00880\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.00880\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00880\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00880\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00880\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00880\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00880\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00880\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00880\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00880\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00880\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00880\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00880\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00880\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.00880 to 0.00767, saving model to .\\model\\lstm_2408_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00767\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00767\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00767\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00767\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00767\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00767\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00767\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.00767 to 0.00764, saving model to .\\model\\lstm_2408_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00764\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00764\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.00764 to 0.00690, saving model to .\\model\\lstm_2408_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00690\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.00690 to 0.00678, saving model to .\\model\\lstm_2408_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00678\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00678\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00678\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00678\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00678\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00678\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00678\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00678\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00678\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00678\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00678\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00678\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00678\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00678\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.00678 to 0.00673, saving model to .\\model\\lstm_2408_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00673\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00673\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00673\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00673\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00673\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00673\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00673\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00673\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00673\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00673\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00673\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00673\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00673\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00673\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00673\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00673\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00673\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00673\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00673\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00673\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00673\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00673\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00673\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00673\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00673\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00673\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00673\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00673\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00673\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00673\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00673\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00673\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00673\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00673\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00673\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00673\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00673\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00673\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00673\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00673\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00673\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00673\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00673\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00673\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00673\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00673\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00673\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.27520, saving model to .\\model\\lstm_2408_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.27520 to 0.16619, saving model to .\\model\\lstm_2408_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.16619 to 0.05738, saving model to .\\model\\lstm_2408_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.05738 to 0.02674, saving model to .\\model\\lstm_2408_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.02674 to 0.02226, saving model to .\\model\\lstm_2408_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.02226 to 0.02125, saving model to .\\model\\lstm_2408_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.02125\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.02125\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.02125 to 0.01351, saving model to .\\model\\lstm_2408_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.01351 to 0.00704, saving model to .\\model\\lstm_2408_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00704\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00704\n",
      "20 2408 trading\n",
      "20 2408 最後資產： 26189 交易報酬率： 23.088300000000007 % 持有報酬率 6.9231 %\n",
      "21 2409 preprocess\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.11871, saving model to .\\model\\lstm_2409_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.11871 to 0.07425, saving model to .\\model\\lstm_2409_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.07425 to 0.02502, saving model to .\\model\\lstm_2409_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.02502 to 0.00242, saving model to .\\model\\lstm_2409_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00242\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00242\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00242\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00242\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00242\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00242\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00242\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.00242\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00242\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00242\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00242\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00242\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00242\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.00242 to 0.00238, saving model to .\\model\\lstm_2409_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.00238 to 0.00230, saving model to .\\model\\lstm_2409_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.00230 to 0.00221, saving model to .\\model\\lstm_2409_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00221\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00221\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00221\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00221\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00221\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.00221 to 0.00218, saving model to .\\model\\lstm_2409_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00218\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00218\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00218\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00218\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00218\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00218\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00218\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00218\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00218\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00218\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00218\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00218\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00218\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00218\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00218\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00218\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00218\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00218\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00218\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00218\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00218\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00218\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00218\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00218\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.00218 to 0.00216, saving model to .\\model\\lstm_2409_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00216\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00216\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00216\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00216\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00216\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00216\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00216\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00216\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00216\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.00216 to 0.00214, saving model to .\\model\\lstm_2409_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00214\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00214\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.00214 to 0.00214, saving model to .\\model\\lstm_2409_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.00214 to 0.00211, saving model to .\\model\\lstm_2409_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00211\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00211\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00211\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00211\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00211\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00211\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00211\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00211\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.00211 to 0.00210, saving model to .\\model\\lstm_2409_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00210\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00210\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00210\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00210\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00210\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00210\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00210\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00210\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00210\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00210\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.00210 to 0.00206, saving model to .\\model\\lstm_2409_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.00206 to 0.00202, saving model to .\\model\\lstm_2409_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00202\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00202\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00202\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00202\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.00202 to 0.00194, saving model to .\\model\\lstm_2409_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.00194 to 0.00190, saving model to .\\model\\lstm_2409_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00190\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00190\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00190\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00190\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.00190 to 0.00185, saving model to .\\model\\lstm_2409_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00185\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00185\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00185\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.09653, saving model to .\\model\\lstm_2409_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.09653 to 0.04110, saving model to .\\model\\lstm_2409_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04110 to 0.00330, saving model to .\\model\\lstm_2409_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.00330\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00330\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00330 to 0.00237, saving model to .\\model\\lstm_2409_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00237\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00237\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00237 to 0.00228, saving model to .\\model\\lstm_2409_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00228\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00228\n",
      "21 2409 trading\n",
      "21 2409 最後資產： 15957 交易報酬率： -25.002099999999995 % 持有報酬率 -18.6235 %\n",
      "22 2412 preprocess\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.11089, saving model to .\\model\\lstm_2412_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.11089 to 0.04881, saving model to .\\model\\lstm_2412_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04881 to 0.00868, saving model to .\\model\\lstm_2412_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.00868 to 0.00364, saving model to .\\model\\lstm_2412_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00364 to 0.00236, saving model to .\\model\\lstm_2412_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.11066, saving model to .\\model\\lstm_2412_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.11066 to 0.04642, saving model to .\\model\\lstm_2412_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04642 to 0.00665, saving model to .\\model\\lstm_2412_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.00665 to 0.00351, saving model to .\\model\\lstm_2412_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00351\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00351\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00351\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00351 to 0.00322, saving model to .\\model\\lstm_2412_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00322\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00322\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00322\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.00322\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00322\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.00322 to 0.00316, saving model to .\\model\\lstm_2412_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00316\n",
      "22 2412 trading\n",
      "22 2412 最後資產： 22824 交易報酬率： 7.272800000000007 % 持有報酬率 2.3256 %\n",
      "23 2603 preprocess\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.15136, saving model to .\\model\\lstm_2603_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.15136 to 0.12140, saving model to .\\model\\lstm_2603_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.12140 to 0.08072, saving model to .\\model\\lstm_2603_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.08072 to 0.03354, saving model to .\\model\\lstm_2603_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.03354 to 0.01888, saving model to .\\model\\lstm_2603_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.01888\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.01888\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.01888 to 0.01313, saving model to .\\model\\lstm_2603_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.01313 to 0.01217, saving model to .\\model\\lstm_2603_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.01217 to 0.01038, saving model to .\\model\\lstm_2603_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.01038 to 0.00782, saving model to .\\model\\lstm_2603_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.00782\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00782 to 0.00657, saving model to .\\model\\lstm_2603_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.00657 to 0.00657, saving model to .\\model\\lstm_2603_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00657\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.11664, saving model to .\\model\\lstm_2603_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.11664 to 0.06155, saving model to .\\model\\lstm_2603_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.06155 to 0.01797, saving model to .\\model\\lstm_2603_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.01797\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.01797\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.01797\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.01797 to 0.01170, saving model to .\\model\\lstm_2603_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.01170\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.01170 to 0.00992, saving model to .\\model\\lstm_2603_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00992 to 0.00847, saving model to .\\model\\lstm_2603_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00847\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00847 to 0.00797, saving model to .\\model\\lstm_2603_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00797\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00797\n",
      "23 2603 trading\n",
      "23 2603 最後資產： 21276 交易報酬率： -0.0027999999999938157 % 持有報酬率 -26.7202 %\n",
      "24 2609 preprocess\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.00648, saving model to .\\model\\lstm_2609_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.00648 to 0.00262, saving model to .\\model\\lstm_2609_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00262\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.00342, saving model to .\\model\\lstm_2609_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00342\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00342\n",
      "24 2609 trading\n",
      "24 2609 最後資產： 12541 交易報酬率： -41.0573 % 持有報酬率 -41.8548 %\n",
      "25 2615 preprocess\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.01204, saving model to .\\model\\lstm_2615_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.01204 to 0.00180, saving model to .\\model\\lstm_2615_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00180 to 0.00142, saving model to .\\model\\lstm_2615_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00142 to 0.00141, saving model to .\\model\\lstm_2615_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00141\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00141\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00141\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00141\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00141 to 0.00084, saving model to .\\model\\lstm_2615_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00084 to 0.00082, saving model to .\\model\\lstm_2615_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.00082 to 0.00078, saving model to .\\model\\lstm_2615_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00078\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00078\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00078\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00078\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00078\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.00078 to 0.00076, saving model to .\\model\\lstm_2615_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.00076 to 0.00076, saving model to .\\model\\lstm_2615_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00076\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.00076 to 0.00074, saving model to .\\model\\lstm_2615_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.00074 to 0.00074, saving model to .\\model\\lstm_2615_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00074\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00074\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00074\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00074\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.00074 to 0.00071, saving model to .\\model\\lstm_2615_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.00071 to 0.00071, saving model to .\\model\\lstm_2615_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00071\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00071\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00071\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00071\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00071\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.00071 to 0.00068, saving model to .\\model\\lstm_2615_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00068\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00068\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00068\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00068\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00068\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00068\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00068\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00068\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00068\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00068\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00068\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00068\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00068\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00068\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00068\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00068\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00068\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00068\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00068\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00068\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00068\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.00068 to 0.00065, saving model to .\\model\\lstm_2615_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00065\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00065\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00065\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00065\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00065\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00065\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00065\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.00065 to 0.00064, saving model to .\\model\\lstm_2615_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00064\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00064\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00064\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.00064 to 0.00063, saving model to .\\model\\lstm_2615_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00063\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00063\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00063\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00063\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00063\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00063\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00063\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00063\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00063\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00063\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00063\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00063\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00063\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.00063 to 0.00061, saving model to .\\model\\lstm_2615_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00061\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.00061 to 0.00059, saving model to .\\model\\lstm_2615_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00059\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00059\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00059\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00059\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00059\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00059\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00059\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00059\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.00059 to 0.00058, saving model to .\\model\\lstm_2615_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00058\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00058\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00058\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00058\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.00058 to 0.00058, saving model to .\\model\\lstm_2615_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.01200, saving model to .\\model\\lstm_2615_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.01200 to 0.00181, saving model to .\\model\\lstm_2615_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.00181\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.00181\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00181\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00181 to 0.00153, saving model to .\\model\\lstm_2615_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00153 to 0.00127, saving model to .\\model\\lstm_2615_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00127\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00127\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00127\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00127\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00127 to 0.00107, saving model to .\\model\\lstm_2615_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00107 to 0.00099, saving model to .\\model\\lstm_2615_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.00099 to 0.00095, saving model to .\\model\\lstm_2615_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00095 to 0.00094, saving model to .\\model\\lstm_2615_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.00094 to 0.00094, saving model to .\\model\\lstm_2615_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.00094 to 0.00094, saving model to .\\model\\lstm_2615_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00094\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.00094 to 0.00092, saving model to .\\model\\lstm_2615_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00092\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.00092 to 0.00090, saving model to .\\model\\lstm_2615_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00090\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00090\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00090\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00090\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00090\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00090\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00090\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00090\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00090\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00090\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00090\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00090\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00090\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.00090 to 0.00086, saving model to .\\model\\lstm_2615_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00086\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00086\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00086\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00086\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00086\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00086\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00086\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00086\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00086\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00086\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00086\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00086\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00086\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00086\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00086\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00086\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00086\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00086\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00086\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00086\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.00086 to 0.00084, saving model to .\\model\\lstm_2615_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00084\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00084\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00084\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00084\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00084\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00084\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00084\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00084\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.00084 to 0.00081, saving model to .\\model\\lstm_2615_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00081\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00081\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00081\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00081\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00081\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00081\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00081\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00081\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00081\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00081\n",
      "25 2615 trading\n",
      "25 2615 最後資產： 21830 交易報酬率： 2.601000000000006 % 持有報酬率 -6.8010 %\n",
      "26 2801 preprocess\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.43364, saving model to .\\model\\lstm_2801_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.43364 to 0.23583, saving model to .\\model\\lstm_2801_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.23583 to 0.05293, saving model to .\\model\\lstm_2801_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.05293 to 0.00374, saving model to .\\model\\lstm_2801_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00374\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.51333, saving model to .\\model\\lstm_2801_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.51333 to 0.37407, saving model to .\\model\\lstm_2801_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.37407 to 0.19151, saving model to .\\model\\lstm_2801_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.19151 to 0.02641, saving model to .\\model\\lstm_2801_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.02641 to 0.00463, saving model to .\\model\\lstm_2801_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00463\n",
      "26 2801 trading\n",
      "26 2801 最後資產： 21276 交易報酬率： -0.0027999999999938157 % 持有報酬率 45.4994 %\n",
      "27 2880 preprocess\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.49030, saving model to .\\model\\lstm_2880_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.49030 to 0.31825, saving model to .\\model\\lstm_2880_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.31825 to 0.11772, saving model to .\\model\\lstm_2880_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.11772 to 0.00401, saving model to .\\model\\lstm_2880_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00401\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.51633, saving model to .\\model\\lstm_2880_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.51633 to 0.36846, saving model to .\\model\\lstm_2880_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36846 to 0.17807, saving model to .\\model\\lstm_2880_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.17807 to 0.01986, saving model to .\\model\\lstm_2880_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.01986 to 0.00364, saving model to .\\model\\lstm_2880_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00364\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00364\n",
      "27 2880 trading\n",
      "27 2880 最後資產： 21276 交易報酬率： -0.0027999999999938157 % 持有報酬率 43.8751 %\n",
      "28 2881 preprocess\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.08555, saving model to .\\model\\lstm_2881_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.08555 to 0.01906, saving model to .\\model\\lstm_2881_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.01906 to 0.00616, saving model to .\\model\\lstm_2881_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.00616 to 0.00511, saving model to .\\model\\lstm_2881_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.00511 to 0.00248, saving model to .\\model\\lstm_2881_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00248\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00248 to 0.00234, saving model to .\\model\\lstm_2881_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00234 to 0.00229, saving model to .\\model\\lstm_2881_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00229\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00229\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00229 to 0.00192, saving model to .\\model\\lstm_2881_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.00192\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00192\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00192\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00192 to 0.00182, saving model to .\\model\\lstm_2881_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00182\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00182\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.00182 to 0.00179, saving model to .\\model\\lstm_2881_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.00179 to 0.00157, saving model to .\\model\\lstm_2881_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00157\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00157\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00157\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00157\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00157\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00157\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00157\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00157\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00157\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00157\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00157\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00157\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00157\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00157\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00157\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00157\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00157\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00157\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00157\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00157\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00157\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00157\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00157\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.00157 to 0.00156, saving model to .\\model\\lstm_2881_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00156\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00156\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.00156 to 0.00155, saving model to .\\model\\lstm_2881_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00155\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00155\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00155\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00155\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00155\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00155\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00155\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00155\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00155\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00155\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00155\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00155\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.00155 to 0.00154, saving model to .\\model\\lstm_2881_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00154\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00154\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00154\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00154\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.00154 to 0.00145, saving model to .\\model\\lstm_2881_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00145\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00145\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.00145 to 0.00139, saving model to .\\model\\lstm_2881_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00139\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00139\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.00139 to 0.00133, saving model to .\\model\\lstm_2881_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00133\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00133\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00133\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00133\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00133\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00133\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00133\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00133\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00133\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00133\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00133\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00133\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00133\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00133\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00133\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00133\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00133\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00133\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00133\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00133\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00133\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.00133 to 0.00125, saving model to .\\model\\lstm_2881_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.11378, saving model to .\\model\\lstm_2881_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.11378 to 0.05310, saving model to .\\model\\lstm_2881_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.05310 to 0.00672, saving model to .\\model\\lstm_2881_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.00672\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.00672 to 0.00273, saving model to .\\model\\lstm_2881_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00273\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00273\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00273 to 0.00180, saving model to .\\model\\lstm_2881_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.00180 to 0.00172, saving model to .\\model\\lstm_2881_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00172\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00172\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00172\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00172\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00172\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00172\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00172\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00172\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00172\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00172\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00172\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00172\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00172\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00172\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00172\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00172\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00172\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00172\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00172\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00172\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00172\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00172\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00172\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00172\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00172\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00172\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00172\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00172\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00172\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00172\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00172\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00172\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00172\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00172\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00172\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00172\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00172\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00172\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00172\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00172\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00172\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00172\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00172\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00172\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00172\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00172\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00172\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00172\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00172\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00172\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00172\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00172\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00172\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00172\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00172\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00172\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00172\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00172\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00172\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00172\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00172\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00172\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00172\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00172\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00172\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00172\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00172\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00172\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00172\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00172\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00172\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00172\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00172\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00172\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00172\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00172\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00172\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00172\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00172\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00172\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00172\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00172\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00172\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00172\n",
      "28 2881 trading\n",
      "28 2881 最後資產： 18999 交易報酬率： -10.704699999999994 % 持有報酬率 -9.0196 %\n",
      "29 2882 preprocess\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.49557, saving model to .\\model\\lstm_2882_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.49557 to 0.37389, saving model to .\\model\\lstm_2882_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.37389 to 0.21299, saving model to .\\model\\lstm_2882_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.21299 to 0.04716, saving model to .\\model\\lstm_2882_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.04716 to 0.00267, saving model to .\\model\\lstm_2882_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00267\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00267\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00267\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00267\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00267\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00267\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.00267\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00267\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00267\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00267\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00267\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.00267 to 0.00231, saving model to .\\model\\lstm_2882_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.00231 to 0.00229, saving model to .\\model\\lstm_2882_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.00229 to 0.00218, saving model to .\\model\\lstm_2882_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.00218 to 0.00211, saving model to .\\model\\lstm_2882_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00211\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00211\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00211\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.00211 to 0.00201, saving model to .\\model\\lstm_2882_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00201\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00201\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.00201 to 0.00194, saving model to .\\model\\lstm_2882_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00194\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00194\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00194\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00194\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00194\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00194\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00194\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00194\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00194\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00194\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00194\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00194\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00194\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00194\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00194\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00194\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00194\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00194\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00194\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00194\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00194\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00194\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00194\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00194\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00194\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00194\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00194\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00194\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00194\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00194\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00194\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00194\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00194\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00194\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00194\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00194\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00194\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00194\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00194\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00194\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00194\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00194\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00194\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00194\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00194\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00194\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00194\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00194\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00194\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00194\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00194\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00194\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00194\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00194\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00194\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00194\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00194\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00194\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00194\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00194\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00194\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00194\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00194\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00194\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00194\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00194\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00194\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00194\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00194\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00194\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00194\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00194\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00194\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.48734, saving model to .\\model\\lstm_2882_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.48734 to 0.34502, saving model to .\\model\\lstm_2882_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.34502 to 0.16663, saving model to .\\model\\lstm_2882_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.16663 to 0.01881, saving model to .\\model\\lstm_2882_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.01881 to 0.00446, saving model to .\\model\\lstm_2882_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00446\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00446\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00446\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00446\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00446\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00446\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00446 to 0.00339, saving model to .\\model\\lstm_2882_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00339\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00339\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00339\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.00339 to 0.00258, saving model to .\\model\\lstm_2882_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00258\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00258\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00258\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00258\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00258\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00258\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.00258 to 0.00258, saving model to .\\model\\lstm_2882_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00258\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00258\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00258\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00258\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00258\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00258\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00258\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00258\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00258\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00258\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00258\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00258\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00258\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00258\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00258\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00258\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00258\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00258\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00258\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00258\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00258\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00258\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00258\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00258\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00258\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00258\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.00258 to 0.00256, saving model to .\\model\\lstm_2882_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00256\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00256\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00256\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00256\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00256\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00256\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00256\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00256\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00256\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00256\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00256\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00256\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00256\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00256\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00256\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00256\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00256\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00256\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00256\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00256\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00256\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00256\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00256\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00256\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00256\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00256\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00256\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00256\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00256\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00256\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00256\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00256\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00256\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00256\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00256\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00256\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00256\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00256\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00256\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00256\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00256\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00256\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00256\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00256\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00256\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00256\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00256\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00256\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00256\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00256\n",
      "29 2882 trading\n",
      "29 2882 最後資產： 18746 交易報酬率： -11.893799999999993 % 持有報酬率 -21.1356 %\n",
      "30 2884 preprocess\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.65295, saving model to .\\model\\lstm_2884_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.65295 to 0.51341, saving model to .\\model\\lstm_2884_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.51341 to 0.31944, saving model to .\\model\\lstm_2884_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.31944 to 0.09959, saving model to .\\model\\lstm_2884_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.09959 to 0.00602, saving model to .\\model\\lstm_2884_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00602\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.65500, saving model to .\\model\\lstm_2884_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.65500 to 0.50066, saving model to .\\model\\lstm_2884_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.50066 to 0.30275, saving model to .\\model\\lstm_2884_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.30275 to 0.08627, saving model to .\\model\\lstm_2884_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.08627 to 0.00537, saving model to .\\model\\lstm_2884_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00537\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00537\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00537\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00537\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00537\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00537\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00537 to 0.00471, saving model to .\\model\\lstm_2884_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00471\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00471\n",
      "30 2884 trading\n",
      "30 2884 最後資產： 21276 交易報酬率： -0.0027999999999938157 % 持有報酬率 67.8013 %\n",
      "31 2885 preprocess\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.12979, saving model to .\\model\\lstm_2885_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.12979 to 0.08881, saving model to .\\model\\lstm_2885_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.08881 to 0.03960, saving model to .\\model\\lstm_2885_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.03960 to 0.00275, saving model to .\\model\\lstm_2885_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00275\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.14366, saving model to .\\model\\lstm_2885_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.14366 to 0.10364, saving model to .\\model\\lstm_2885_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.10364 to 0.05418, saving model to .\\model\\lstm_2885_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.05418 to 0.00679, saving model to .\\model\\lstm_2885_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00679\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00679\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00679 to 0.00162, saving model to .\\model\\lstm_2885_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00162\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00162\n",
      "31 2885 trading\n",
      "31 2885 最後資產： 21398 交易報酬率： 0.5706000000000062 % 持有報酬率 46.3768 %\n",
      "32 2886 preprocess\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.22169, saving model to .\\model\\lstm_2886_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.22169 to 0.11145, saving model to .\\model\\lstm_2886_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.11145 to 0.01361, saving model to .\\model\\lstm_2886_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.01361\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.01361\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.01361 to 0.00329, saving model to .\\model\\lstm_2886_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00329\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00329\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00329\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00329\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00329\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00329 to 0.00311, saving model to .\\model\\lstm_2886_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00311 to 0.00272, saving model to .\\model\\lstm_2886_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00272\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00272 to 0.00240, saving model to .\\model\\lstm_2886_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00240\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.00240 to 0.00232, saving model to .\\model\\lstm_2886_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.00232 to 0.00216, saving model to .\\model\\lstm_2886_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00216\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.00216 to 0.00212, saving model to .\\model\\lstm_2886_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00212\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.00212 to 0.00206, saving model to .\\model\\lstm_2886_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00206\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.00206 to 0.00204, saving model to .\\model\\lstm_2886_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.23605, saving model to .\\model\\lstm_2886_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.23605 to 0.13977, saving model to .\\model\\lstm_2886_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.13977 to 0.03298, saving model to .\\model\\lstm_2886_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.03298 to 0.02228, saving model to .\\model\\lstm_2886_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.02228\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.02228 to 0.00781, saving model to .\\model\\lstm_2886_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00781 to 0.00605, saving model to .\\model\\lstm_2886_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00605\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00605 to 0.00555, saving model to .\\model\\lstm_2886_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00555 to 0.00386, saving model to .\\model\\lstm_2886_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00386\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.00386\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00386 to 0.00334, saving model to .\\model\\lstm_2886_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00334\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00334 to 0.00329, saving model to .\\model\\lstm_2886_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.00329 to 0.00304, saving model to .\\model\\lstm_2886_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00304\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.00304 to 0.00294, saving model to .\\model\\lstm_2886_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00294\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00294\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00294\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00294\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00294\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00294\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00294\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00294\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00294\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00294\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00294\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00294\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.00294 to 0.00279, saving model to .\\model\\lstm_2886_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00279\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00279\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00279\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00279\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00279\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00279\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00279\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00279\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00279\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00279\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00279\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00279\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00279\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00279\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00279\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00279\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00279\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00279\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00279\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.00279 to 0.00278, saving model to .\\model\\lstm_2886_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00278\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00278\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.00278 to 0.00276, saving model to .\\model\\lstm_2886_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00276\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00276\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00276\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00276\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00276\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.00276 to 0.00274, saving model to .\\model\\lstm_2886_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00274\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00274\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00274\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.00274 to 0.00268, saving model to .\\model\\lstm_2886_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00268\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00268\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.00268 to 0.00259, saving model to .\\model\\lstm_2886_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00259\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00259\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00259\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00259\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.00259 to 0.00256, saving model to .\\model\\lstm_2886_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00256\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00256\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00256\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00256\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00256\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00256\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00256\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00256\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00256\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00256\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00256\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00256\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.00256 to 0.00253, saving model to .\\model\\lstm_2886_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.00253 to 0.00250, saving model to .\\model\\lstm_2886_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00250\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00250\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00250\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.00250 to 0.00250, saving model to .\\model\\lstm_2886_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.00250 to 0.00248, saving model to .\\model\\lstm_2886_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00248\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.00248 to 0.00248, saving model to .\\model\\lstm_2886_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00248\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00248\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00248\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.00248 to 0.00246, saving model to .\\model\\lstm_2886_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.00246 to 0.00246, saving model to .\\model\\lstm_2886_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00246\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.00246 to 0.00243, saving model to .\\model\\lstm_2886_5x22_5d_trainbest.h5\n",
      "32 2886 trading\n",
      "32 2886 最後資產： 21356 交易報酬率： 0.3732000000000062 % 持有報酬率 26.4463 %\n",
      "33 2887 preprocess\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.55678, saving model to .\\model\\lstm_2887_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.55678 to 0.44301, saving model to .\\model\\lstm_2887_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.44301 to 0.29446, saving model to .\\model\\lstm_2887_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.29446 to 0.13469, saving model to .\\model\\lstm_2887_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.13469 to 0.03677, saving model to .\\model\\lstm_2887_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.03677 to 0.02128, saving model to .\\model\\lstm_2887_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.02128\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.02128\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.02128\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.02128\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.02128\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.02128\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.02128\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.02128\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.02128\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.02128\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.02128 to 0.01771, saving model to .\\model\\lstm_2887_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.01771 to 0.01235, saving model to .\\model\\lstm_2887_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.01235 to 0.01001, saving model to .\\model\\lstm_2887_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.01001\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.01001\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.01001 to 0.00953, saving model to .\\model\\lstm_2887_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00953\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00953\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00953\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00953\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00953\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00953\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00953\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00953\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00953\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00953\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00953\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00953\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00953\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00953\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00953\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00953\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00953\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00953\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00953\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00953\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00953\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00953\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00953\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00953\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00953\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00953\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00953\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00953\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00953\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00953\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00953\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00953\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00953\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00953\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00953\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00953\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00953\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00953\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00953\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00953\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00953\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00953\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00953\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00953\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00953\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00953\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00953\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00953\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00953\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00953\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00953\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00953\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00953\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00953\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00953\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00953\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00953\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00953\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00953\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00953\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00953\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00953\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00953\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00953\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00953\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00953\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00953\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00953\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00953\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00953\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00953\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00953\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00953\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00953\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00953\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00953\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00953\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00953\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.52127, saving model to .\\model\\lstm_2887_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.52127 to 0.34443, saving model to .\\model\\lstm_2887_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.34443 to 0.13417, saving model to .\\model\\lstm_2887_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.13417 to 0.00993, saving model to .\\model\\lstm_2887_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.00993 to 0.00839, saving model to .\\model\\lstm_2887_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00839\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00839\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00839\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00839\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00839\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00839\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.00839\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00839\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00839\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00839\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00839\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.00839 to 0.00680, saving model to .\\model\\lstm_2887_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00680\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00680\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.00680 to 0.00648, saving model to .\\model\\lstm_2887_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.00648 to 0.00629, saving model to .\\model\\lstm_2887_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00629\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00629\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00629\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00629\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00629\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00629\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00629\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00629\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00629\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00629\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00629\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00629\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00629\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00629\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00629\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00629\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00629\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00629\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00629\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00629\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00629\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00629\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00629\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00629\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00629\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00629\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00629\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00629\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00629\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00629\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00629\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00629\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00629\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00629\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00629\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00629\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00629\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00629\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00629\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00629\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00629\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00629\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00629\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00629\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00629\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00629\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00629\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00629\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00629\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00629\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00629\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00629\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00629\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00629\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00629\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00629\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00629\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00629\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00629\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00629\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00629\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00629\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00629\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00629\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00629\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00629\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00629\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00629\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00629\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00629\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00629\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00629\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00629\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00629\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00629\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00629\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00629\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00629\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00629\n",
      "33 2887 trading\n",
      "33 2887 最後資產： 21276 交易報酬率： -0.0027999999999938157 % 持有報酬率 10.4130 %\n",
      "34 2891 preprocess\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.41925, saving model to .\\model\\lstm_2891_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.41925 to 0.20013, saving model to .\\model\\lstm_2891_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.20013 to 0.02344, saving model to .\\model\\lstm_2891_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.02344 to 0.00461, saving model to .\\model\\lstm_2891_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00461\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00461\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00461\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00461\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00461\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00461\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00461\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.00461\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00461\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.00461 to 0.00230, saving model to .\\model\\lstm_2891_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00230 to 0.00134, saving model to .\\model\\lstm_2891_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.00134 to 0.00130, saving model to .\\model\\lstm_2891_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00130\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.44537, saving model to .\\model\\lstm_2891_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.44537 to 0.28112, saving model to .\\model\\lstm_2891_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.28112 to 0.10266, saving model to .\\model\\lstm_2891_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.10266 to 0.00673, saving model to .\\model\\lstm_2891_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.00673 to 0.00389, saving model to .\\model\\lstm_2891_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00389\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00389\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00389\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00389\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00389\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00389\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.00389\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00389\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00389\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00389 to 0.00330, saving model to .\\model\\lstm_2891_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.00330 to 0.00183, saving model to .\\model\\lstm_2891_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.00183 to 0.00179, saving model to .\\model\\lstm_2891_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00179\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00179\n",
      "34 2891 trading\n",
      "34 2891 最後資產： 21702 交易報酬率： 1.9994000000000063 % 持有報酬率 9.5354 %\n",
      "35 2892 preprocess\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.54496, saving model to .\\model\\lstm_2892_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.54496 to 0.41090, saving model to .\\model\\lstm_2892_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.41090 to 0.23664, saving model to .\\model\\lstm_2892_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.23664 to 0.08058, saving model to .\\model\\lstm_2892_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.08058 to 0.06332, saving model to .\\model\\lstm_2892_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.06332\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.06332\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.06332\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.06332\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.06332\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.06332 to 0.04688, saving model to .\\model\\lstm_2892_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.04688\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.04688\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.04688 to 0.03786, saving model to .\\model\\lstm_2892_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.03786 to 0.02381, saving model to .\\model\\lstm_2892_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.02381 to 0.02093, saving model to .\\model\\lstm_2892_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.53756, saving model to .\\model\\lstm_2892_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.53756 to 0.37490, saving model to .\\model\\lstm_2892_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.37490 to 0.18414, saving model to .\\model\\lstm_2892_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.18414 to 0.06363, saving model to .\\model\\lstm_2892_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.06363\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.06363\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.06363\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.06363\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.06363\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.06363 to 0.06191, saving model to .\\model\\lstm_2892_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.06191 to 0.04495, saving model to .\\model\\lstm_2892_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.04495 to 0.04467, saving model to .\\model\\lstm_2892_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.04467 to 0.04436, saving model to .\\model\\lstm_2892_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.04436 to 0.03037, saving model to .\\model\\lstm_2892_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.03037 to 0.01728, saving model to .\\model\\lstm_2892_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.01728 to 0.01424, saving model to .\\model\\lstm_2892_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.01424\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.01424\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.01424\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.01424\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.01424\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.01424\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.01424\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.01424\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.01424\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.01424\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.01424\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.01424\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.01424\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.01424\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.01424\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.01424\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.01424\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.01424\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.01424\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.01424\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.01424\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.01424\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.01424\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.01424\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.01424\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.01424\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.01424\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.01424\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.01424\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.01424\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.01424\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.01424\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.01424\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.01424\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.01424\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.01424\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.01424\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.01424\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.01424\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.01424\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.01424\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.01424\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.01424\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.01424\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.01424\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.01424\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.01424\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.01424\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.01424\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.01424\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.01424\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.01424\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.01424\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.01424\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.01424\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.01424\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.01424\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.01424\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.01424\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.01424\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.01424\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.01424\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.01424\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.01424\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.01424\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.01424\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.01424\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.01424\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.01424\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.01424\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.01424\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.01424\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.01424\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.01424\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.01424\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.01424\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.01424\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.01424\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.01424\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.01424\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.01424\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.01424\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.01424\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.01424\n",
      "35 2892 trading\n",
      "35 2892 最後資產： 21276 交易報酬率： -0.0027999999999938157 % 持有報酬率 23.6643 %\n",
      "36 2912 preprocess\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.51285, saving model to .\\model\\lstm_2912_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.51285 to 0.38679, saving model to .\\model\\lstm_2912_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.38679 to 0.21915, saving model to .\\model\\lstm_2912_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.21915 to 0.05678, saving model to .\\model\\lstm_2912_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.05678 to 0.01753, saving model to .\\model\\lstm_2912_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.01753\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.01753\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.01753\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.01753\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.01753\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.01753\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.01753\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.01753\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.01753\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.01753\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.01753\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.01753 to 0.01751, saving model to .\\model\\lstm_2912_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.01751 to 0.01544, saving model to .\\model\\lstm_2912_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.01544\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.01544 to 0.01460, saving model to .\\model\\lstm_2912_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.01460 to 0.01303, saving model to .\\model\\lstm_2912_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.01303 to 0.01302, saving model to .\\model\\lstm_2912_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.01302\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.01302\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.01302\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.01302\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.01302\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.01302 to 0.01183, saving model to .\\model\\lstm_2912_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.01183\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.01183\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.01183 to 0.01138, saving model to .\\model\\lstm_2912_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.01138\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.01138\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.01138\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.01138\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.01138\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.01138\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.01138\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.01138\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.01138\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.01138\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.01138\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.01138\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.01138\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.01138\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.01138\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.01138\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.01138\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.01138\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.01138\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.01138\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.01138\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.01138\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.01138\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.01138\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.01138\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.01138\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.01138\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.01138\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.01138\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.01138\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.01138\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.01138\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.01138\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.01138\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.01138\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.01138\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.01138\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.01138\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.01138\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.01138\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.01138\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.01138\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.01138\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.01138\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.01138\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.01138\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.01138\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.01138\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.01138\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.01138\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.01138\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.01138\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.01138\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.01138\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.01138\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.01138 to 0.01133, saving model to .\\model\\lstm_2912_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.01133\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.01133\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.01133\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.01133\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.01133\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.01133\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.01133\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.01133\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.01133\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.01133\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.01133\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.01133\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.01133\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.52703, saving model to .\\model\\lstm_2912_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.52703 to 0.39374, saving model to .\\model\\lstm_2912_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.39374 to 0.22790, saving model to .\\model\\lstm_2912_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.22790 to 0.05917, saving model to .\\model\\lstm_2912_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.05917 to 0.01303, saving model to .\\model\\lstm_2912_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.01303\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.01303\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.01303\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.01303\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.01303\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.01303\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.01303\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.01303\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.01303\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.01303\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.01303\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.01303 to 0.01162, saving model to .\\model\\lstm_2912_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.01162\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.01162\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.01162\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.01162 to 0.01132, saving model to .\\model\\lstm_2912_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.01132\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.01132\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.01132\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.01132\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.01132\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.01132\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.01132\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.01132\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.01132\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.01132\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.01132\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.01132\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.01132\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.01132\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.01132\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.01132\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.01132\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.01132\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.01132\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.01132\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.01132\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.01132\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.01132\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.01132\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.01132\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.01132\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.01132\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.01132\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.01132\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.01132\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.01132\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.01132\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.01132\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.01132\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.01132\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.01132\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.01132\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.01132\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.01132\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.01132\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.01132\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.01132\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.01132\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.01132\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.01132\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.01132\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.01132\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.01132\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.01132\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.01132\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.01132\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.01132\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.01132\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.01132\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.01132\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.01132\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.01132\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.01132\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.01132\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.01132\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.01132\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.01132\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.01132\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.01132\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.01132\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.01132\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.01132\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.01132\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.01132\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.01132\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.01132\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.01132\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.01132\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.01132\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.01132\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.01132\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.01132\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.01132\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.01132\n",
      "36 2912 trading\n",
      "36 2912 最後資產： 21276 交易報酬率： -0.0027999999999938157 % 持有報酬率 5.7391 %\n",
      "37 3008 preprocess\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.52321, saving model to .\\model\\lstm_3008_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.52321 to 0.35393, saving model to .\\model\\lstm_3008_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.35393 to 0.16201, saving model to .\\model\\lstm_3008_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.16201 to 0.06839, saving model to .\\model\\lstm_3008_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.06839\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.06839\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.06839\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.06839\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.06839 to 0.02603, saving model to .\\model\\lstm_3008_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.02603 to 0.01673, saving model to .\\model\\lstm_3008_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.01673\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.01673\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.01673\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.01673 to 0.01471, saving model to .\\model\\lstm_3008_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.01471\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.01471\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.01471\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.01471\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.01471\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.01471\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.01471\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.01471 to 0.01411, saving model to .\\model\\lstm_3008_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.01411\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.01411\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.01411 to 0.01400, saving model to .\\model\\lstm_3008_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.01400\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.01400\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.01400\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.01400\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.01400\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.01400\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.01400\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.01400\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.01400\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.01400\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.01400\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.01400\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.01400\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.01400\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.01400\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.01400\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.01400\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.01400\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.01400\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.01400\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.01400\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.01400\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.01400\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.01400\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.01400\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.01400\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.01400\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.01400\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.01400\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.01400\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.01400\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.01400\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.01400\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.01400\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.01400\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.01400\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.01400\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.01400\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.01400\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.01400\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.01400\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.01400\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.01400\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.01400\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.01400\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.01400\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.01400\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.01400\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.01400\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.01400\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.01400\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.01400\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.01400\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.01400\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.01400\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.01400\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.01400\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.01400\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.01400\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.01400\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.01400\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.01400\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.01400\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.01400\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.01400\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.01400\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.01400\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.01400\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.01400\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.01400\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.01400\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.01400\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.01400\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.01400\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.01400\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.56396, saving model to .\\model\\lstm_3008_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.56396 to 0.43121, saving model to .\\model\\lstm_3008_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.43121 to 0.25419, saving model to .\\model\\lstm_3008_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.25419 to 0.10759, saving model to .\\model\\lstm_3008_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.10759 to 0.10399, saving model to .\\model\\lstm_3008_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.10399\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.10399\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.10399\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.10399 to 0.04672, saving model to .\\model\\lstm_3008_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.04672 to 0.02671, saving model to .\\model\\lstm_3008_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.02671\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.02671\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.02671 to 0.02468, saving model to .\\model\\lstm_3008_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.02468 to 0.02015, saving model to .\\model\\lstm_3008_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.02015\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.02015\n",
      "37 3008 trading\n",
      "37 3008 最後資產： 32855 交易報酬率： 54.4185 % 持有報酬率 17.5088 %\n",
      "38 3034 preprocess\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.00844, saving model to .\\model\\lstm_3034_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.00844 to 0.00163, saving model to .\\model\\lstm_3034_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.00163\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.00163\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00163\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00163\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00163\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00163\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00163\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00163\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00163\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.00163\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00163\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00163\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00163\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00163\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00163\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00163\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00163\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00163\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00163\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00163\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00163\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00163\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00163\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00163\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00163\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00163\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00163\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00163\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00163\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00163\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00163\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00163\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00163\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00163\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00163\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00163\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00163\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00163\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00163\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00163\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00163\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00163\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00163\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00163\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00163\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00163\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00163\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00163\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00163\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00163\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00163\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00163\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00163\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00163\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00163\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.00163 to 0.00159, saving model to .\\model\\lstm_3034_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00159\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00159\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00159\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00159\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00159\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00159\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00159\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00159\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00159\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.00159 to 0.00149, saving model to .\\model\\lstm_3034_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.00149 to 0.00134, saving model to .\\model\\lstm_3034_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00134\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00134\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00134\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00134\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00134\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00134\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00134\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00134\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.00134 to 0.00125, saving model to .\\model\\lstm_3034_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00125\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.00125 to 0.00123, saving model to .\\model\\lstm_3034_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00123\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00123\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.01213, saving model to .\\model\\lstm_3034_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.01213 to 0.00297, saving model to .\\model\\lstm_3034_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.00297\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.00297\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00297\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00297\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00297\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00297\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00297\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00297\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00297\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.00297\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00297\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00297\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00297\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00297\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00297\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00297\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00297\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00297\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00297\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00297\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00297\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00297\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00297\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00297\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00297\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00297\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00297\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00297\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.00297 to 0.00272, saving model to .\\model\\lstm_3034_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.00272 to 0.00243, saving model to .\\model\\lstm_3034_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00243\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00243\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00243\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00243\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00243\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00243\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00243\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00243\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00243\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00243\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.00243 to 0.00234, saving model to .\\model\\lstm_3034_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00234\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00234\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.00234 to 0.00206, saving model to .\\model\\lstm_3034_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00206\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00206\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.00206 to 0.00203, saving model to .\\model\\lstm_3034_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00203\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00203\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.00203 to 0.00192, saving model to .\\model\\lstm_3034_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00192\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00192\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00192\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00192\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.00192 to 0.00187, saving model to .\\model\\lstm_3034_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00187\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00187\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00187\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00187\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00187\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.00187 to 0.00180, saving model to .\\model\\lstm_3034_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00180\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.00180 to 0.00174, saving model to .\\model\\lstm_3034_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00174\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00174\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00174\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00174\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00174\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00174\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00174\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00174\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00174\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00174\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00174\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00174\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.00174 to 0.00166, saving model to .\\model\\lstm_3034_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00166\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00166\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00166\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00166\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00166\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00166\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00166\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00166\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00166\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00166\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00166\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00166\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00166\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00166\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00166\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00166\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00166\n",
      "38 3034 trading\n",
      "38 3034 最後資產： 37518 交易報酬率： 76.33460000000001 % 持有報酬率 90.4348 %\n",
      "39 3045 preprocess\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.42104, saving model to .\\model\\lstm_3045_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.42104 to 0.29184, saving model to .\\model\\lstm_3045_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.29184 to 0.14131, saving model to .\\model\\lstm_3045_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.14131 to 0.01853, saving model to .\\model\\lstm_3045_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.01853 to 0.00962, saving model to .\\model\\lstm_3045_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00962 to 0.00464, saving model to .\\model\\lstm_3045_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00464\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00464\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00464\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00464\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00464\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00464 to 0.00462, saving model to .\\model\\lstm_3045_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00462\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00462\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00462\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00462\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.00462 to 0.00383, saving model to .\\model\\lstm_3045_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00383\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.00383 to 0.00366, saving model to .\\model\\lstm_3045_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.00366 to 0.00362, saving model to .\\model\\lstm_3045_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.00362 to 0.00358, saving model to .\\model\\lstm_3045_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00358\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.00358 to 0.00356, saving model to .\\model\\lstm_3045_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.00356 to 0.00341, saving model to .\\model\\lstm_3045_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.00341 to 0.00339, saving model to .\\model\\lstm_3045_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00339\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00339\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00339\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00339\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00339\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00339\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00339\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00339\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00339\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00339\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00339\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00339\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00339\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00339\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00339\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00339\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00339\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00339\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00339\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00339\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00339\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00339\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00339\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00339\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00339\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00339\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00339\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00339\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00339\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00339\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00339\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00339\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00339\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00339\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00339\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00339\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00339\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00339\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00339\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00339\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00339\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00339\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00339\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00339\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00339\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00339\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00339\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00339\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00339\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00339\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00339\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00339\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00339\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00339\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00339\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00339\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00339\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00339\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00339\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00339\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00339\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00339\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00339\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00339\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00339\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00339\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00339\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00339\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00339\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00339\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00339\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00339\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00339\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00339\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00339\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.42197, saving model to .\\model\\lstm_3045_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.42197 to 0.30116, saving model to .\\model\\lstm_3045_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.30116 to 0.16056, saving model to .\\model\\lstm_3045_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.16056 to 0.03587, saving model to .\\model\\lstm_3045_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.03587 to 0.00700, saving model to .\\model\\lstm_3045_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00700 to 0.00645, saving model to .\\model\\lstm_3045_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00645\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00645\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00645\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00645\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00645\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00645 to 0.00613, saving model to .\\model\\lstm_3045_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00613 to 0.00522, saving model to .\\model\\lstm_3045_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00522\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00522\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00522\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.00522 to 0.00468, saving model to .\\model\\lstm_3045_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00468\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00468\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.00468 to 0.00452, saving model to .\\model\\lstm_3045_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.00452 to 0.00440, saving model to .\\model\\lstm_3045_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.00440 to 0.00438, saving model to .\\model\\lstm_3045_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00438\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00438\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.00438 to 0.00430, saving model to .\\model\\lstm_3045_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00430\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00430\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00430\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.00430 to 0.00428, saving model to .\\model\\lstm_3045_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00428\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00428\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00428\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00428\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00428\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00428\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00428\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00428\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00428\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00428\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00428\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00428\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00428\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00428\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00428\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00428\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00428\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00428\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00428\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00428\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00428\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00428\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00428\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00428\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00428\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00428\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00428\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00428\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00428\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00428\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00428\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00428\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00428\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00428\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00428\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00428\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00428\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00428\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00428\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00428\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00428\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00428\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00428\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00428\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00428\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00428\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00428\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00428\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00428\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00428\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00428\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00428\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00428\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00428\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00428\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00428\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00428\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00428\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00428\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00428\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00428\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00428\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00428\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00428\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00428\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00428\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00428\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00428\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00428\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00428\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00428\n",
      "39 3045 trading\n",
      "39 3045 最後資產： 20752 交易報酬率： -2.465599999999994 % 持有報酬率 3.7037 %\n",
      "40 4904 preprocess\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.34299, saving model to .\\model\\lstm_4904_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.34299 to 0.24640, saving model to .\\model\\lstm_4904_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.24640 to 0.13129, saving model to .\\model\\lstm_4904_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.13129 to 0.02404, saving model to .\\model\\lstm_4904_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.02404 to 0.00864, saving model to .\\model\\lstm_4904_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00864 to 0.00521, saving model to .\\model\\lstm_4904_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00521\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00521\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00521\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00521\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00521 to 0.00464, saving model to .\\model\\lstm_4904_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00464 to 0.00437, saving model to .\\model\\lstm_4904_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00437 to 0.00421, saving model to .\\model\\lstm_4904_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.00421 to 0.00384, saving model to .\\model\\lstm_4904_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00384\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.00384 to 0.00358, saving model to .\\model\\lstm_4904_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00358\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00358\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00358\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.00358 to 0.00354, saving model to .\\model\\lstm_4904_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00354\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00354\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.00354 to 0.00335, saving model to .\\model\\lstm_4904_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.00335 to 0.00322, saving model to .\\model\\lstm_4904_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00322\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00322\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00322\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.00322 to 0.00317, saving model to .\\model\\lstm_4904_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00317\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00317\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00317\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00317\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00317\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00317\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00317\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00317\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00317\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00317\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.00317 to 0.00316, saving model to .\\model\\lstm_4904_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00316\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36212, saving model to .\\model\\lstm_4904_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36212 to 0.28242, saving model to .\\model\\lstm_4904_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.28242 to 0.17807, saving model to .\\model\\lstm_4904_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.17807 to 0.06295, saving model to .\\model\\lstm_4904_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.06295 to 0.00524, saving model to .\\model\\lstm_4904_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00524\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00524\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00524\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00524\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00524\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00524\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.00524\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00524 to 0.00463, saving model to .\\model\\lstm_4904_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.00463 to 0.00445, saving model to .\\model\\lstm_4904_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00445 to 0.00444, saving model to .\\model\\lstm_4904_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.00444 to 0.00416, saving model to .\\model\\lstm_4904_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00416\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00416\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00416\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.00416 to 0.00378, saving model to .\\model\\lstm_4904_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00378\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00378\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00378\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.00378 to 0.00357, saving model to .\\model\\lstm_4904_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.00357 to 0.00354, saving model to .\\model\\lstm_4904_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00354\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00354\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.00354 to 0.00344, saving model to .\\model\\lstm_4904_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.00344 to 0.00343, saving model to .\\model\\lstm_4904_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00343\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00343\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.00343 to 0.00341, saving model to .\\model\\lstm_4904_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.00341 to 0.00341, saving model to .\\model\\lstm_4904_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00341\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00341\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00341\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00341\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00341\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00341\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00341\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00341\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00341\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00341\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00341\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00341\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00341\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00341\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00341\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00341\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00341\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00341\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00341\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00341\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00341\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00341\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00341\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00341\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00341\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00341\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00341\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00341\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00341\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00341\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00341\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00341\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00341\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00341\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00341\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00341\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00341\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00341\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00341\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00341\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00341\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00341\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00341\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00341\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00341\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00341\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00341\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00341\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00341\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00341\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00341\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00341\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00341\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00341\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00341\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00341\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00341\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00341\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00341\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00341\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00341\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00341\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00341\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00341\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00341\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00341\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00341\n",
      "40 4904 trading\n",
      "40 4904 最後資產： 23528 交易報酬率： 10.581600000000007 % 持有報酬率 -2.0380 %\n",
      "41 4938 preprocess\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.48885, saving model to .\\model\\lstm_4938_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.48885 to 0.33921, saving model to .\\model\\lstm_4938_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.33921 to 0.16371, saving model to .\\model\\lstm_4938_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.16371 to 0.02714, saving model to .\\model\\lstm_4938_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.02714 to 0.01455, saving model to .\\model\\lstm_4938_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.01455 to 0.01181, saving model to .\\model\\lstm_4938_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.01181\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.01181\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.01181\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.01181 to 0.01072, saving model to .\\model\\lstm_4938_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.01072 to 0.00883, saving model to .\\model\\lstm_4938_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00883 to 0.00833, saving model to .\\model\\lstm_4938_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00833\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00833\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00833\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00833\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00833\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00833\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00833\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00833\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00833\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.00833 to 0.00820, saving model to .\\model\\lstm_4938_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.00820 to 0.00817, saving model to .\\model\\lstm_4938_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00817\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00817\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.00817 to 0.00792, saving model to .\\model\\lstm_4938_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.00792 to 0.00765, saving model to .\\model\\lstm_4938_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.00765 to 0.00753, saving model to .\\model\\lstm_4938_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00753\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00753\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.00753 to 0.00697, saving model to .\\model\\lstm_4938_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.00697 to 0.00658, saving model to .\\model\\lstm_4938_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00658\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00658\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00658\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.00658 to 0.00640, saving model to .\\model\\lstm_4938_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00640\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00640\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00640\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.00640 to 0.00624, saving model to .\\model\\lstm_4938_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.00624 to 0.00608, saving model to .\\model\\lstm_4938_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00608\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00608\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00608\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.00608 to 0.00605, saving model to .\\model\\lstm_4938_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.00605 to 0.00591, saving model to .\\model\\lstm_4938_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00591\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00591\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.00591 to 0.00569, saving model to .\\model\\lstm_4938_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00569\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00569\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00569\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.00569 to 0.00554, saving model to .\\model\\lstm_4938_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00554\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00554\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.00554 to 0.00553, saving model to .\\model\\lstm_4938_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.00553 to 0.00539, saving model to .\\model\\lstm_4938_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00539\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00539\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00539\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00539\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00539\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00539\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00539\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00539\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00539\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00539\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00539\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00539\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00539\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00539\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00539\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00539\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00539\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00539\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00539\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00539\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00539\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00539\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00539\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00539\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00539\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00539\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00539\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00539\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00539\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00539\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00539\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00539\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.00539 to 0.00503, saving model to .\\model\\lstm_4938_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00503\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00503\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00503\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00503\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00503\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00503\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00503\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00503\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00503\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00503\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.49821, saving model to .\\model\\lstm_4938_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.49821 to 0.34882, saving model to .\\model\\lstm_4938_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.34882 to 0.17492, saving model to .\\model\\lstm_4938_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.17492 to 0.03577, saving model to .\\model\\lstm_4938_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.03577 to 0.01552, saving model to .\\model\\lstm_4938_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.01552\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.01552\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.01552\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.01552\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.01552 to 0.01267, saving model to .\\model\\lstm_4938_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.01267 to 0.01091, saving model to .\\model\\lstm_4938_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.01091 to 0.01025, saving model to .\\model\\lstm_4938_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.01025\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.01025\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.01025\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.01025\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.01025\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.01025\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.01025\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.01025\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.01025\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.01025\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.01025\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.01025\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.01025\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.01025\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.01025\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.01025\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.01025 to 0.00994, saving model to .\\model\\lstm_4938_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.00994 to 0.00973, saving model to .\\model\\lstm_4938_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.00973 to 0.00956, saving model to .\\model\\lstm_4938_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00956\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.00956 to 0.00945, saving model to .\\model\\lstm_4938_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.00945 to 0.00935, saving model to .\\model\\lstm_4938_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.00935 to 0.00912, saving model to .\\model\\lstm_4938_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00912\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00912\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00912\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.00912 to 0.00897, saving model to .\\model\\lstm_4938_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.00897 to 0.00888, saving model to .\\model\\lstm_4938_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.00888 to 0.00849, saving model to .\\model\\lstm_4938_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00849\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00849\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.00849 to 0.00822, saving model to .\\model\\lstm_4938_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00822\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.00822 to 0.00818, saving model to .\\model\\lstm_4938_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.00818 to 0.00802, saving model to .\\model\\lstm_4938_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.00802 to 0.00763, saving model to .\\model\\lstm_4938_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00763\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00763\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00763\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.00763 to 0.00758, saving model to .\\model\\lstm_4938_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00758\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.00758 to 0.00738, saving model to .\\model\\lstm_4938_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00738\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.00738 to 0.00733, saving model to .\\model\\lstm_4938_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.00733 to 0.00721, saving model to .\\model\\lstm_4938_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00721\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00721\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00721\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00721\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00721\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.00721 to 0.00720, saving model to .\\model\\lstm_4938_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00720\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00720\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.00720 to 0.00691, saving model to .\\model\\lstm_4938_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00691\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00691\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.00691 to 0.00666, saving model to .\\model\\lstm_4938_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00666\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00666\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00666\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00666\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00666\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00666\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00666\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00666\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00666\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00666\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00666\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00666\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00666\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00666\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00666\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00666\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00666\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00666\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00666\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00666\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00666\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.00666 to 0.00635, saving model to .\\model\\lstm_4938_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00635\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00635\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00635\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00635\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00635\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00635\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.00635 to 0.00634, saving model to .\\model\\lstm_4938_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00634\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00634\n",
      "41 4938 trading\n",
      "41 4938 最後資產： 20118 交易報酬率： -5.445399999999994 % 持有報酬率 -5.1318 %\n",
      "42 5880 preprocess\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.40887, saving model to .\\model\\lstm_5880_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.40887 to 0.23372, saving model to .\\model\\lstm_5880_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.23372 to 0.08424, saving model to .\\model\\lstm_5880_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.08424 to 0.06931, saving model to .\\model\\lstm_5880_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.06931\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.06931\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.06931\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.06931\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.06931\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.06931\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.06931\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.06931\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.06931\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.06931 to 0.05692, saving model to .\\model\\lstm_5880_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.05692 to 0.03851, saving model to .\\model\\lstm_5880_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.03851 to 0.03122, saving model to .\\model\\lstm_5880_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.03122 to 0.03040, saving model to .\\model\\lstm_5880_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.03040 to 0.02757, saving model to .\\model\\lstm_5880_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.02757 to 0.02431, saving model to .\\model\\lstm_5880_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.02431\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.02431\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.02431\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.02431\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.02431\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.02431\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.02431\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.02431\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.02431\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.02431\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.02431\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.02431\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.02431\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.02431\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.02431\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.02431\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.02431\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.02431\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.02431\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.02431\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.02431\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.02431\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.02431\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.02431\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.02431\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.02431\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.02431\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.02431\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.02431\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.02431\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.02431\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.02431\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.02431\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.02431\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.02431\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.02431\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.02431\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.02431\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.02431\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.02431\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.02431\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.02431\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.02431\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.02431\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.02431\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.02431\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.02431\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.02431\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.02431\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.02431\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.02431\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.02431\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.02431\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.02431\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.02431\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.02431\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.02431\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.02431\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.02431\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.02431\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.02431\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.02431\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.02431\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.02431\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.02431\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.02431\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.02431\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.02431\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.02431\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.02431\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.02431\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.02431\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.02431\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.02431\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.02431\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.02431\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.02431\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.02431\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.02431\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.02431\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.02431\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.49527, saving model to .\\model\\lstm_5880_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.49527 to 0.36403, saving model to .\\model\\lstm_5880_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36403 to 0.21301, saving model to .\\model\\lstm_5880_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.21301 to 0.08940, saving model to .\\model\\lstm_5880_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.08940 to 0.07233, saving model to .\\model\\lstm_5880_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.07233\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.07233\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.07233\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.07233\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.07233\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.07233\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.07233\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.07233\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.07233\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.07233\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.07233\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.07233 to 0.06281, saving model to .\\model\\lstm_5880_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.06281 to 0.05291, saving model to .\\model\\lstm_5880_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.05291 to 0.04973, saving model to .\\model\\lstm_5880_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.04973 to 0.04275, saving model to .\\model\\lstm_5880_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.04275 to 0.03628, saving model to .\\model\\lstm_5880_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.03628 to 0.03567, saving model to .\\model\\lstm_5880_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.03567\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.03567\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.03567\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.03567\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.03567\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.03567\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.03567\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.03567\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.03567\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.03567\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.03567\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.03567\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.03567\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.03567\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.03567\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.03567\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.03567\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.03567\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.03567\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.03567\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.03567\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.03567\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.03567\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.03567\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.03567\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.03567\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.03567\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.03567\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.03567\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.03567\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.03567\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.03567\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.03567\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.03567\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.03567\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.03567\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.03567\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.03567\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.03567\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.03567\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.03567\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.03567\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.03567\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.03567\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.03567\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.03567\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.03567\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.03567\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.03567\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.03567\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.03567\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.03567\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.03567\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.03567\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.03567\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.03567\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.03567\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.03567\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.03567\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.03567\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.03567\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.03567\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.03567\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.03567\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.03567\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.03567\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.03567\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.03567\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.03567\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.03567\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.03567\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.03567\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.03567\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.03567\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.03567\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.03567\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.03567\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.03567\n",
      "42 5880 trading\n",
      "42 5880 最後資產： 21276 交易報酬率： -0.0027999999999938157 % 持有報酬率 32.2143 %\n",
      "43 6415 preprocess\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.59700, saving model to .\\model\\lstm_6415_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.59700 to 0.47264, saving model to .\\model\\lstm_6415_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.47264 to 0.32036, saving model to .\\model\\lstm_6415_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.32036 to 0.17136, saving model to .\\model\\lstm_6415_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.17136 to 0.11755, saving model to .\\model\\lstm_6415_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.11755\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.11755\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.11755\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.11755\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.11755\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.11755\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.11755 to 0.09889, saving model to .\\model\\lstm_6415_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.09889 to 0.08477, saving model to .\\model\\lstm_6415_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.08477 to 0.06093, saving model to .\\model\\lstm_6415_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.06093 to 0.03163, saving model to .\\model\\lstm_6415_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.03163 to 0.02322, saving model to .\\model\\lstm_6415_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.02322\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.02322\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.02322\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.02322\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.02322\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.02322\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.02322\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.02322\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.02322\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.02322\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.02322\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.02322\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.02322\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.02322\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.02322\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.02322\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.02322\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.02322\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.02322\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.02322\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.02322\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.02322\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.02322\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.02322\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.02322\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.02322\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.02322\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.02322\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.02322\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.02322\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.02322\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.02322\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.02322\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.02322\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.02322\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.02322\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.02322\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.02322\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.02322\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.02322\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.02322\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.02322\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.02322\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.02322\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.02322\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.02322\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.02322\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.02322\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.02322\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.02322\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.02322\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.02322\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.02322\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.02322\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.02322\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.02322\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.02322\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.02322\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.02322\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.02322\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.02322\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.02322\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.02322\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.02322\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.02322\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.02322\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.02322\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.02322\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.02322\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.02322\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.02322\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.02322\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.02322\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.02322\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.02322\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.02322\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.02322\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.02322\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.02322\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.02322\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.02322\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.02322\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.02322\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.02322\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.53158, saving model to .\\model\\lstm_6415_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.53158 to 0.35078, saving model to .\\model\\lstm_6415_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.35078 to 0.14437, saving model to .\\model\\lstm_6415_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.14437 to 0.07071, saving model to .\\model\\lstm_6415_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.07071\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.07071\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.07071\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.07071\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.07071\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.07071 to 0.05075, saving model to .\\model\\lstm_6415_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.05075 to 0.03821, saving model to .\\model\\lstm_6415_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.03821 to 0.02872, saving model to .\\model\\lstm_6415_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.02872 to 0.01912, saving model to .\\model\\lstm_6415_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.01912 to 0.01052, saving model to .\\model\\lstm_6415_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.01052\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.01052\n",
      "43 6415 trading\n",
      "43 6415 最後資產： 29417 交易報酬率： 38.25990000000001 % 持有報酬率 38.2824 %\n",
      "44 6505 preprocess\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.50733, saving model to .\\model\\lstm_6505_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.50733 to 0.34852, saving model to .\\model\\lstm_6505_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.34852 to 0.15169, saving model to .\\model\\lstm_6505_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.15169 to 0.05311, saving model to .\\model\\lstm_6505_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.05311\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.05311\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.05311\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.05311\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.05311 to 0.05157, saving model to .\\model\\lstm_6505_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.05157 to 0.02633, saving model to .\\model\\lstm_6505_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.02633\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.02633\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.02633 to 0.01238, saving model to .\\model\\lstm_6505_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.01238 to 0.00627, saving model to .\\model\\lstm_6505_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00627\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00627\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00627\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00627\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00627\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00627\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.00627 to 0.00549, saving model to .\\model\\lstm_6505_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.00549 to 0.00371, saving model to .\\model\\lstm_6505_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.00371 to 0.00361, saving model to .\\model\\lstm_6505_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00361\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00361\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.00361 to 0.00286, saving model to .\\model\\lstm_6505_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00286\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00286\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00286\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00286\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00286\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00286\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00286\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00286\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00286\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00286\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00286\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00286\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00286\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00286\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00286\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00286\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00286\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.00286 to 0.00265, saving model to .\\model\\lstm_6505_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00265\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00265\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00265\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00265\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00265\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00265\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00265\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.00265 to 0.00261, saving model to .\\model\\lstm_6505_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00261\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00261\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00261\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00261\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00261\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00261\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00261\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00261\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00261\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00261\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00261\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.00261 to 0.00236, saving model to .\\model\\lstm_6505_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00236\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.00236 to 0.00232, saving model to .\\model\\lstm_6505_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00232\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00232\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00232\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00232\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00232\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00232\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00232\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00232\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00232\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00232\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00232\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00232\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00232\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00232\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00232\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00232\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00232\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00232\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00232\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00232\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00232\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00232\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00232\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00232\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00232\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00232\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00232\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.49136, saving model to .\\model\\lstm_6505_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.49136 to 0.32214, saving model to .\\model\\lstm_6505_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.32214 to 0.13393, saving model to .\\model\\lstm_6505_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.13393 to 0.07335, saving model to .\\model\\lstm_6505_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.07335\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.07335\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.07335\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.07335\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.07335 to 0.04657, saving model to .\\model\\lstm_6505_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.04657 to 0.02627, saving model to .\\model\\lstm_6505_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.02627\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.02627 to 0.02100, saving model to .\\model\\lstm_6505_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.02100 to 0.00870, saving model to .\\model\\lstm_6505_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.00870 to 0.00628, saving model to .\\model\\lstm_6505_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00628\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00628\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00628\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.00628 to 0.00569, saving model to .\\model\\lstm_6505_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00569\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.00569 to 0.00472, saving model to .\\model\\lstm_6505_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.00472 to 0.00293, saving model to .\\model\\lstm_6505_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.00293 to 0.00281, saving model to .\\model\\lstm_6505_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00281\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00281\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.00281 to 0.00277, saving model to .\\model\\lstm_6505_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00277\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00277\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00277\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00277\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00277\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00277\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.00277 to 0.00271, saving model to .\\model\\lstm_6505_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00271\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00271\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00271\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00271\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00271\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00271\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00271\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00271\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00271\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00271\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00271\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00271\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00271\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00271\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00271\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00271\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00271\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00271\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00271\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00271\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00271\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00271\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00271\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.00271 to 0.00255, saving model to .\\model\\lstm_6505_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00255\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00255\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00255\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00255\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00255\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00255\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00255\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.00255 to 0.00248, saving model to .\\model\\lstm_6505_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00248\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00248\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00248\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00248\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00248\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00248\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00248\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00248\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00248\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00248\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00248\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00248\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00248\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00248\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00248\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00248\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00248\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00248\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00248\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00248\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00248\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00248\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00248\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00248\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00248\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00248\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00248\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00248\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00248\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00248\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00248\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00248\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00248\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00248\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00248\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00248\n",
      "44 6505 trading\n",
      "44 6505 最後資產： 24039 交易報酬率： 12.983300000000007 % 持有報酬率 -15.2174 %\n",
      "45 8046 preprocess\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.00240, saving model to .\\model\\lstm_8046_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.00240 to 0.00129, saving model to .\\model\\lstm_8046_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.00129\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.00129\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00129\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00129\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00129\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00129\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00129\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00129\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00129\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.00129\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00129\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.00129 to 0.00113, saving model to .\\model\\lstm_8046_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00113 to 0.00089, saving model to .\\model\\lstm_8046_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.00089 to 0.00068, saving model to .\\model\\lstm_8046_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.00068 to 0.00055, saving model to .\\model\\lstm_8046_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.00395, saving model to .\\model\\lstm_8046_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.00395 to 0.00056, saving model to .\\model\\lstm_8046_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00056\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00056\n",
      "45 8046 trading\n",
      "45 8046 最後資產： 55181 交易報酬率： 159.35070000000005 % 持有報酬率 68.6239 %\n",
      "46 8454 preprocess\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04153, saving model to .\\model\\lstm_8454_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04153 to 0.01552, saving model to .\\model\\lstm_8454_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.01552 to 0.00155, saving model to .\\model\\lstm_8454_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.00155\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00155\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00155\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00155\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00155\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00155\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00155\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00155\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.00155\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00155\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.00155 to 0.00122, saving model to .\\model\\lstm_8454_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00122 to 0.00112, saving model to .\\model\\lstm_8454_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00112\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00112\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.00112 to 0.00111, saving model to .\\model\\lstm_8454_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00111\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00111\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00111\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00111\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00111\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00111\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00111\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00111\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00111\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00111\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00111\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00111\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00111\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00111\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00111\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00111\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00111\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00111\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00111\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00111\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00111\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00111\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00111\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00111\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00111\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00111\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00111\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00111\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00111\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00111\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00111\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00111\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00111\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00111\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00111\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00111\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00111\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00111\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00111\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00111\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00111\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00111\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00111\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00111\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00111\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00111\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00111\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00111\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00111\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00111\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00111\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00111\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00111\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00111\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00111\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00111\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00111\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00111\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00111\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00111\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00111\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00111\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00111\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00111\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00111\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00111\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00111\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00111\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00111\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00111\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00111\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00111\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00111\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00111\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00111\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00111\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00111\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00111\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00111\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00111\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00111\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00111\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.05525, saving model to .\\model\\lstm_8454_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.05525 to 0.03596, saving model to .\\model\\lstm_8454_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.03596 to 0.01761, saving model to .\\model\\lstm_8454_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.01761 to 0.00344, saving model to .\\model\\lstm_8454_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00344\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00344\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00344\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00344\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00344 to 0.00259, saving model to .\\model\\lstm_8454_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00259 to 0.00191, saving model to .\\model\\lstm_8454_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00191\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.00191\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00191\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00191\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00191\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00191\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00191\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.00191 to 0.00138, saving model to .\\model\\lstm_8454_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.00138 to 0.00124, saving model to .\\model\\lstm_8454_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00124\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00124\n",
      "46 8454 trading\n",
      "46 8454 最後資產： 21025 交易報酬率： -1.182499999999994 % 持有報酬率 31.8182 %\n",
      "47 9910 preprocess\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.34947, saving model to .\\model\\lstm_9910_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.34947 to 0.24633, saving model to .\\model\\lstm_9910_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.24633 to 0.12245, saving model to .\\model\\lstm_9910_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.12245 to 0.01985, saving model to .\\model\\lstm_9910_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.01985 to 0.00780, saving model to .\\model\\lstm_9910_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00780\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00780 to 0.00235, saving model to .\\model\\lstm_9910_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00235\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00235\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00235 to 0.00204, saving model to .\\model\\lstm_9910_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00204\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.00204\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00204\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00204\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00204\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00204\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00204\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00204\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00204\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00204\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00204\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00204\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00204\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00204\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00204\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00204\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00204\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00204\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.00204 to 0.00186, saving model to .\\model\\lstm_9910_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.00186 to 0.00153, saving model to .\\model\\lstm_9910_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00153\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00153\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.00153 to 0.00128, saving model to .\\model\\lstm_9910_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00128\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00128\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.00128 to 0.00113, saving model to .\\model\\lstm_9910_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.00113 to 0.00113, saving model to .\\model\\lstm_9910_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00113\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.00113 to 0.00110, saving model to .\\model\\lstm_9910_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.00110 to 0.00106, saving model to .\\model\\lstm_9910_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00106\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00106\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00106\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00106\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.00106 to 0.00102, saving model to .\\model\\lstm_9910_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00102\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00102\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.00102 to 0.00097, saving model to .\\model\\lstm_9910_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.00097 to 0.00091, saving model to .\\model\\lstm_9910_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00091\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00091\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00091\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00091\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00091\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00091\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.00091 to 0.00089, saving model to .\\model\\lstm_9910_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00089\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00089\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00089\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00089\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00089\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00089\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00089\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00089\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00089\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.00089 to 0.00088, saving model to .\\model\\lstm_9910_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00088\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00088\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00088\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00088\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00088\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00088\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00088\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.00088 to 0.00088, saving model to .\\model\\lstm_9910_5x22_3d_trainbest.h5\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00088\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00088\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00088\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00088\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00088\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00088\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00088\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00088\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00088\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00088\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00088\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00088\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00088\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00088\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00088\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00088\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00088\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00088\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00088\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00088\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00088\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00088\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00088\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00088\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00088\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00088\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.33956, saving model to .\\model\\lstm_9910_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.33956 to 0.23171, saving model to .\\model\\lstm_9910_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.23171 to 0.09610, saving model to .\\model\\lstm_9910_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.09610 to 0.00317, saving model to .\\model\\lstm_9910_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00317\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00317\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00317\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00317\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00317 to 0.00306, saving model to .\\model\\lstm_9910_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00306\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00306\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.00306\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00306\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00306\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00306\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00306\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00306\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00306\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00306\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00306\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00306\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00306\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.00306 to 0.00306, saving model to .\\model\\lstm_9910_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.00306 to 0.00270, saving model to .\\model\\lstm_9910_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00270\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00270\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.00270 to 0.00267, saving model to .\\model\\lstm_9910_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.00267 to 0.00256, saving model to .\\model\\lstm_9910_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.00256 to 0.00217, saving model to .\\model\\lstm_9910_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00217\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00217\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.00217 to 0.00193, saving model to .\\model\\lstm_9910_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.00193 to 0.00171, saving model to .\\model\\lstm_9910_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00171\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.00171 to 0.00168, saving model to .\\model\\lstm_9910_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.00168 to 0.00160, saving model to .\\model\\lstm_9910_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00160\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.00160 to 0.00157, saving model to .\\model\\lstm_9910_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00157\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.00157 to 0.00141, saving model to .\\model\\lstm_9910_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.00141 to 0.00136, saving model to .\\model\\lstm_9910_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00136\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00136\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.00136 to 0.00130, saving model to .\\model\\lstm_9910_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.00130 to 0.00126, saving model to .\\model\\lstm_9910_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00126\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.00126 to 0.00124, saving model to .\\model\\lstm_9910_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.00124 to 0.00121, saving model to .\\model\\lstm_9910_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00121\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00121\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.00121 to 0.00119, saving model to .\\model\\lstm_9910_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00119\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00119\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00119\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00119\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.00119 to 0.00119, saving model to .\\model\\lstm_9910_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00119\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.00119 to 0.00119, saving model to .\\model\\lstm_9910_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00119\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00119\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00119\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00119\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00119\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00119\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.00119 to 0.00118, saving model to .\\model\\lstm_9910_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00118\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00118\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00118\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00118\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00118\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00118\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00118\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00118\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00118\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.00118 to 0.00118, saving model to .\\model\\lstm_9910_5x22_5d_trainbest.h5\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00118\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00118\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00118\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00118\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00118\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00118\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00118\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00118\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00118\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00118\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00118\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00118\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00118\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00118\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00118\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00118\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00118\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00118\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00118\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00118\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00118\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00118\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00118\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00118\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00118\n",
      "47 9910 trading\n",
      "47 9910 最後資產： 21707 交易報酬率： 2.022900000000006 % 持有報酬率 57.1429 %\n",
      "\n",
      "總投資報酬率： -97.8293 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock</th>\n",
       "      <th>trading ror</th>\n",
       "      <th>buyhold ror</th>\n",
       "      <th>win</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1101</td>\n",
       "      <td>0.528</td>\n",
       "      <td>40.739</td>\n",
       "      <td>False</td>\n",
       "      <td>-40.211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1216</td>\n",
       "      <td>2.719</td>\n",
       "      <td>10.417</td>\n",
       "      <td>False</td>\n",
       "      <td>-7.698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1301</td>\n",
       "      <td>3.236</td>\n",
       "      <td>1.733</td>\n",
       "      <td>True</td>\n",
       "      <td>1.503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1303</td>\n",
       "      <td>-4.599</td>\n",
       "      <td>-5.455</td>\n",
       "      <td>True</td>\n",
       "      <td>0.855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1326</td>\n",
       "      <td>-6.508</td>\n",
       "      <td>-15.459</td>\n",
       "      <td>True</td>\n",
       "      <td>8.951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1402</td>\n",
       "      <td>11.056</td>\n",
       "      <td>11.381</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1590</td>\n",
       "      <td>-1.864</td>\n",
       "      <td>-10.803</td>\n",
       "      <td>True</td>\n",
       "      <td>8.939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2002</td>\n",
       "      <td>-6.573</td>\n",
       "      <td>-3.239</td>\n",
       "      <td>False</td>\n",
       "      <td>-3.335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2207</td>\n",
       "      <td>21.890</td>\n",
       "      <td>90.251</td>\n",
       "      <td>False</td>\n",
       "      <td>-68.361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2303</td>\n",
       "      <td>-6.414</td>\n",
       "      <td>15.439</td>\n",
       "      <td>False</td>\n",
       "      <td>-21.852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2308</td>\n",
       "      <td>15.592</td>\n",
       "      <td>7.447</td>\n",
       "      <td>True</td>\n",
       "      <td>8.145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2317</td>\n",
       "      <td>-9.375</td>\n",
       "      <td>-23.537</td>\n",
       "      <td>True</td>\n",
       "      <td>14.162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2324</td>\n",
       "      <td>-6.794</td>\n",
       "      <td>-10.664</td>\n",
       "      <td>True</td>\n",
       "      <td>3.869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2327</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>47.903</td>\n",
       "      <td>False</td>\n",
       "      <td>-47.906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2330</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>42.366</td>\n",
       "      <td>False</td>\n",
       "      <td>-42.368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2357</td>\n",
       "      <td>-3.453</td>\n",
       "      <td>-16.727</td>\n",
       "      <td>True</td>\n",
       "      <td>13.274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2379</td>\n",
       "      <td>16.461</td>\n",
       "      <td>115.596</td>\n",
       "      <td>False</td>\n",
       "      <td>-99.135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2382</td>\n",
       "      <td>-6.550</td>\n",
       "      <td>3.877</td>\n",
       "      <td>False</td>\n",
       "      <td>-10.427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2395</td>\n",
       "      <td>9.816</td>\n",
       "      <td>40.465</td>\n",
       "      <td>False</td>\n",
       "      <td>-30.650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2408</td>\n",
       "      <td>23.088</td>\n",
       "      <td>6.923</td>\n",
       "      <td>True</td>\n",
       "      <td>16.165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2409</td>\n",
       "      <td>-25.002</td>\n",
       "      <td>-18.623</td>\n",
       "      <td>False</td>\n",
       "      <td>-6.379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2412</td>\n",
       "      <td>7.273</td>\n",
       "      <td>2.326</td>\n",
       "      <td>True</td>\n",
       "      <td>4.947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2603</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-26.720</td>\n",
       "      <td>True</td>\n",
       "      <td>26.717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2609</td>\n",
       "      <td>-41.057</td>\n",
       "      <td>-41.855</td>\n",
       "      <td>True</td>\n",
       "      <td>0.798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2615</td>\n",
       "      <td>2.601</td>\n",
       "      <td>-6.801</td>\n",
       "      <td>True</td>\n",
       "      <td>9.402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2801</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>45.499</td>\n",
       "      <td>False</td>\n",
       "      <td>-45.502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2880</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>43.875</td>\n",
       "      <td>False</td>\n",
       "      <td>-43.878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2881</td>\n",
       "      <td>-10.705</td>\n",
       "      <td>-9.020</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2882</td>\n",
       "      <td>-11.894</td>\n",
       "      <td>-21.136</td>\n",
       "      <td>True</td>\n",
       "      <td>9.242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2884</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>67.801</td>\n",
       "      <td>False</td>\n",
       "      <td>-67.804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2885</td>\n",
       "      <td>0.571</td>\n",
       "      <td>46.377</td>\n",
       "      <td>False</td>\n",
       "      <td>-45.806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2886</td>\n",
       "      <td>0.373</td>\n",
       "      <td>26.446</td>\n",
       "      <td>False</td>\n",
       "      <td>-26.073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2887</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>10.413</td>\n",
       "      <td>False</td>\n",
       "      <td>-10.416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2891</td>\n",
       "      <td>1.999</td>\n",
       "      <td>9.535</td>\n",
       "      <td>False</td>\n",
       "      <td>-7.536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2892</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>23.664</td>\n",
       "      <td>False</td>\n",
       "      <td>-23.667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2912</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>5.739</td>\n",
       "      <td>False</td>\n",
       "      <td>-5.742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>3008</td>\n",
       "      <td>54.419</td>\n",
       "      <td>17.509</td>\n",
       "      <td>True</td>\n",
       "      <td>36.910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>3034</td>\n",
       "      <td>76.335</td>\n",
       "      <td>90.435</td>\n",
       "      <td>False</td>\n",
       "      <td>-14.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>3045</td>\n",
       "      <td>-2.466</td>\n",
       "      <td>3.704</td>\n",
       "      <td>False</td>\n",
       "      <td>-6.169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>4904</td>\n",
       "      <td>10.582</td>\n",
       "      <td>-2.038</td>\n",
       "      <td>True</td>\n",
       "      <td>12.620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>4938</td>\n",
       "      <td>-5.445</td>\n",
       "      <td>-5.132</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>5880</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>32.214</td>\n",
       "      <td>False</td>\n",
       "      <td>-32.217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>6415</td>\n",
       "      <td>38.260</td>\n",
       "      <td>38.282</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>6505</td>\n",
       "      <td>12.983</td>\n",
       "      <td>-15.217</td>\n",
       "      <td>True</td>\n",
       "      <td>28.201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>8046</td>\n",
       "      <td>159.351</td>\n",
       "      <td>68.624</td>\n",
       "      <td>True</td>\n",
       "      <td>90.727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>8454</td>\n",
       "      <td>-1.182</td>\n",
       "      <td>31.818</td>\n",
       "      <td>False</td>\n",
       "      <td>-33.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>9910</td>\n",
       "      <td>2.023</td>\n",
       "      <td>57.143</td>\n",
       "      <td>False</td>\n",
       "      <td>-55.120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   stock  trading ror  buyhold ror    win    diff\n",
       "0   1101        0.528       40.739  False -40.211\n",
       "1   1216        2.719       10.417  False  -7.698\n",
       "2   1301        3.236        1.733   True   1.503\n",
       "3   1303       -4.599       -5.455   True   0.855\n",
       "4   1326       -6.508      -15.459   True   8.951\n",
       "5   1402       11.056       11.381  False  -0.324\n",
       "6   1590       -1.864      -10.803   True   8.939\n",
       "7   2002       -6.573       -3.239  False  -3.335\n",
       "8   2207       21.890       90.251  False -68.361\n",
       "9   2303       -6.414       15.439  False -21.852\n",
       "10  2308       15.592        7.447   True   8.145\n",
       "11  2317       -9.375      -23.537   True  14.162\n",
       "12  2324       -6.794      -10.664   True   3.869\n",
       "13  2327       -0.003       47.903  False -47.906\n",
       "14  2330       -0.003       42.366  False -42.368\n",
       "15  2357       -3.453      -16.727   True  13.274\n",
       "16  2379       16.461      115.596  False -99.135\n",
       "17  2382       -6.550        3.877  False -10.427\n",
       "18  2395        9.816       40.465  False -30.650\n",
       "19  2408       23.088        6.923   True  16.165\n",
       "20  2409      -25.002      -18.623  False  -6.379\n",
       "21  2412        7.273        2.326   True   4.947\n",
       "22  2603       -0.003      -26.720   True  26.717\n",
       "23  2609      -41.057      -41.855   True   0.798\n",
       "24  2615        2.601       -6.801   True   9.402\n",
       "25  2801       -0.003       45.499  False -45.502\n",
       "26  2880       -0.003       43.875  False -43.878\n",
       "27  2881      -10.705       -9.020  False  -1.685\n",
       "28  2882      -11.894      -21.136   True   9.242\n",
       "29  2884       -0.003       67.801  False -67.804\n",
       "30  2885        0.571       46.377  False -45.806\n",
       "31  2886        0.373       26.446  False -26.073\n",
       "32  2887       -0.003       10.413  False -10.416\n",
       "33  2891        1.999        9.535  False  -7.536\n",
       "34  2892       -0.003       23.664  False -23.667\n",
       "35  2912       -0.003        5.739  False  -5.742\n",
       "36  3008       54.419       17.509   True  36.910\n",
       "37  3034       76.335       90.435  False -14.100\n",
       "38  3045       -2.466        3.704  False  -6.169\n",
       "39  4904       10.582       -2.038   True  12.620\n",
       "40  4938       -5.445       -5.132  False  -0.314\n",
       "41  5880       -0.003       32.214  False -32.217\n",
       "42  6415       38.260       38.282  False  -0.022\n",
       "43  6505       12.983      -15.217   True  28.201\n",
       "44  8046      159.351       68.624   True  90.727\n",
       "45  8454       -1.182       31.818  False -33.001\n",
       "46  9910        2.023       57.143  False -55.120"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1h 32min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print_time(colab)\n",
    "status(time_slide, feature, days)\n",
    "# Load the TensorBoard notebook extension\n",
    "%reload_ext tensorboard\n",
    "\n",
    "counter = 1\n",
    "compare_ror = []\n",
    "train_start = '2013-01-01'\n",
    "train_end = '2017-12-31'\n",
    "# stockNum = ['2412']\n",
    "\n",
    "for stock_num in stockNum:\n",
    "    tf.keras.backend.clear_session()\n",
    "    print(counter,str(stock_num),\"preprocess\")\n",
    "    # Short Term Model\n",
    "    day_st = 3\n",
    "    X_st, y_st, y_st_ori, scaler_x_st, scaler_y_st, _, _ = data_preprocess(stock_num, train_start, train_end, feature, days = day_st)\n",
    "    train_x_st, train_y_st, val_x_st, val_y_st, price_y_st_tra, price_y_st = input_build(X_st, y_st, y_st_ori)\n",
    "    model_st = lstm4_reg_v1(train_x_st)\n",
    "    model_st_name = f'lstm_{stock_num}_{train_x_st.shape[1]}x{train_x_st.shape[2]}_{day_st}d_trainbest.h5'\n",
    "    history = model_st.fit(train_x_st, train_y_st , \n",
    "        verbose=0, epochs = 100, batch_size = batch_size, \n",
    "        validation_data=(val_x_st, val_y_st), \n",
    "        callbacks = callback(os.path.join(gd_root,model_root,model_st_name)))\n",
    "    model_st_history = pd.DataFrame(history.history)\n",
    "\n",
    "    # Long Term Model\n",
    "    day_lt = 5\n",
    "    X_lt, y_lt, y_lt_ori, scaler_x_lt, scaler_y_lt, _, _ = data_preprocess(stock_num, train_start, train_end, feature, days = day_lt)\n",
    "    train_x_lt, train_y_lt, val_x_lt, val_y_lt, price_y_lt_tra, price_y_lt = input_build(X_lt, y_lt, y_lt_ori)\n",
    "    model_lt = lstm4_reg_v1(train_x_lt)\n",
    "    model_lt_name = f'lstm_{stock_num}_{train_x_lt.shape[1]}x{train_x_lt.shape[2]}_{day_lt}d_trainbest.h5'\n",
    "    history = model_lt.fit(train_x_lt, train_y_lt , \n",
    "        verbose=0, epochs = 100, batch_size = batch_size, \n",
    "        validation_data=(val_x_lt, val_y_lt), \n",
    "        callbacks = callback(os.path.join(gd_root,model_root,model_lt_name)))\n",
    "    model_lt_history = pd.DataFrame(history.history)\n",
    "    # print(train_x_st.shape,train_y_st.shape,val_x_st.shape,val_y_st.shape)\n",
    "    # print(train_x_lt.shape,train_y_lt.shape,val_x_lt.shape,val_y_lt.shape)\n",
    "    \n",
    "    # Save Model    \n",
    "    # print(model_st_name)\n",
    "    # print(model_lt_name)\n",
    "    model_st.save(os.path.join(gd_root,model_root,model_st_name))\n",
    "    model_lt.save(os.path.join(gd_root,model_root,model_lt_name))\n",
    "\n",
    "\n",
    "# setting backtest model\n",
    "\n",
    "    test_start = '2018-01-01'\n",
    "    test_end = '2019-12-31'\n",
    "    # stockNum = ['2412']\n",
    "    # setting the information of data\n",
    "    asset_at_the_start = 1000000\n",
    "    total_asset_present_value = 0\n",
    "    record_ror = []\n",
    "    trading_ROR = {}\n",
    "    portfolio_list = []\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "    print(counter,str(stock_num),\"trading\")\n",
    "    # Prepare Data\n",
    "    # todo modify data preprocess, separate scaler & long short terms \n",
    "    day = 1\n",
    "    X, y, y_ori, _, _, df, df_y = data_preprocess(stock_num, test_start, test_end, feature, days = day, train=False,scaler_x=scaler_x_st,scaler_y=scaler_y_st)\n",
    "\n",
    "# start backtest evaluaion \n",
    "    left_money = asset_at_the_start*(1/len(stockNum))\n",
    "    own_asset = left_money \n",
    "    own_stock = 0 \n",
    "    # money_record = [left_money]*time_slide\n",
    "    stock_record = [[x,left_money] for x in df.index[:time_slide]]\n",
    "    # origin_hold_stock = 0 \n",
    "    X = np.array(X)\n",
    "    day_st = 3\n",
    "    day_lt = 5\n",
    "    model_st_name = f'lstm_{stock_num}_{X.shape[1]}x{X.shape[2]}_{day_st}d_trainbest.h5'\n",
    "    model_lt_name = f'lstm_{stock_num}_{X.shape[1]}x{X.shape[2]}_{day_lt}d_trainbest.h5'\n",
    "    model_st = tf.keras.models.load_model(os.path.join(gd_root,model_root,model_st_name))\n",
    "    model_lt = tf.keras.models.load_model(os.path.join(gd_root,model_root,model_lt_name))\n",
    "    # print(os.path.join(gd_root,model_root,model_st_name))\n",
    "    # print(os.path.join(gd_root,model_root,model_lt_name))\n",
    "    df_trade = df[time_slide:]\n",
    "    for window in range(len(X)):\n",
    "#         print(model_st.predict(X[window]))\n",
    "        price_s = scaler_y_st.inverse_transform(model_st.predict(X[window][np.newaxis,:]))\n",
    "        price_l = scaler_y_lt.inverse_transform(model_lt.predict(X[window][np.newaxis,:]))\n",
    "        predict = (price_s,price_l)\n",
    "        price = df_trade['Close'][window]\n",
    "        stock_assets, own_stock, left_money, action, pred, price = highlow_strategy(price_s, own_stock, left_money, price)\n",
    "        stock_record.append([df_trade.index[window], stock_assets, action, pred, price])\n",
    "    \n",
    "    daily_assets = pd.DataFrame(stock_record, columns = ['date','assets','action','pred_price','close_price']).set_index('date')  \n",
    "    portfolio_list.append(daily_assets)\n",
    "    last_money = stock_record[-1][1]\n",
    "    left_assets = int(last_money)-(own_asset)\n",
    "    record_ror.append([str(stock_num), int(last_money), left_assets/own_asset])\n",
    "    \n",
    "    # compare to buy & hold strategy\n",
    "    bench_df = pdr.DataReader(str(stock_num)+'.tw', 'yahoo', start=test_start, end=test_end)\n",
    "    buyhold_ror = ((bench_df['Close'][-1]/bench_df['Close'][0])-1)*100\n",
    "    stock_ror = (left_assets*100/(own_asset))\n",
    "\n",
    "    print(f\"{str(counter).zfill(2)} {str(stock_num)} 最後資產： {int(last_money)} 交易報酬率： {stock_ror} % 持有報酬率 {buyhold_ror:.4f} %\")\n",
    "    \n",
    "    total_asset_present_value += int(last_money)\n",
    "    trading_ROR[stock_num] = stock_ror\n",
    "    compare_ror.append([stock_num, stock_ror, buyhold_ror])\n",
    "    counter+=1\n",
    "\n",
    "trading_df = pd.DataFrame(compare_ror,columns=['stock','trading ror','buyhold ror'])\n",
    "trading_df['win'] = trading_df['trading ror'] > trading_df['buyhold ror']\n",
    "trading_df['diff'] = trading_df['trading ror'] - trading_df['buyhold ror']\n",
    "\n",
    "print(\"\\n總投資報酬率：\", (total_asset_present_value - asset_at_the_start)*100 / asset_at_the_start, \"%\\n\")\n",
    "\n",
    "Valid_Portfolio = portfolio_list[0].join(portfolio_list[1:])\n",
    "Valid_Portfolio['total'] = Valid_Portfolio.sum(axis=1)\n",
    "save_pickle(Valid_Portfolio, os.path.join(gd_root, ror_root, 'Valid_Portfolio.pkl'))\n",
    "save_pickle(trading_ROR, os.path.join(gd_root, ror_root, 'trading_ROR.pkl'))\n",
    "ror_df = pd.DataFrame(trading_ROR.items(), columns = ['Stock','trading_ROR'])\n",
    "ror_df.to_csv(os.path.join(gd_root, ror_root, 'trading_ROR.csv'))\n",
    "display(trading_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock</th>\n",
       "      <th>trading ror</th>\n",
       "      <th>buyhold ror</th>\n",
       "      <th>win</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1301</td>\n",
       "      <td>3.236</td>\n",
       "      <td>1.733</td>\n",
       "      <td>True</td>\n",
       "      <td>1.503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1303</td>\n",
       "      <td>-4.599</td>\n",
       "      <td>-5.455</td>\n",
       "      <td>True</td>\n",
       "      <td>0.855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1326</td>\n",
       "      <td>-6.508</td>\n",
       "      <td>-15.459</td>\n",
       "      <td>True</td>\n",
       "      <td>8.951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1590</td>\n",
       "      <td>-1.864</td>\n",
       "      <td>-10.803</td>\n",
       "      <td>True</td>\n",
       "      <td>8.939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2308</td>\n",
       "      <td>15.592</td>\n",
       "      <td>7.447</td>\n",
       "      <td>True</td>\n",
       "      <td>8.145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2317</td>\n",
       "      <td>-9.375</td>\n",
       "      <td>-23.537</td>\n",
       "      <td>True</td>\n",
       "      <td>14.162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2324</td>\n",
       "      <td>-6.794</td>\n",
       "      <td>-10.664</td>\n",
       "      <td>True</td>\n",
       "      <td>3.869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2357</td>\n",
       "      <td>-3.453</td>\n",
       "      <td>-16.727</td>\n",
       "      <td>True</td>\n",
       "      <td>13.274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2408</td>\n",
       "      <td>23.088</td>\n",
       "      <td>6.923</td>\n",
       "      <td>True</td>\n",
       "      <td>16.165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2412</td>\n",
       "      <td>7.273</td>\n",
       "      <td>2.326</td>\n",
       "      <td>True</td>\n",
       "      <td>4.947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2603</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-26.720</td>\n",
       "      <td>True</td>\n",
       "      <td>26.717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2609</td>\n",
       "      <td>-41.057</td>\n",
       "      <td>-41.855</td>\n",
       "      <td>True</td>\n",
       "      <td>0.798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2615</td>\n",
       "      <td>2.601</td>\n",
       "      <td>-6.801</td>\n",
       "      <td>True</td>\n",
       "      <td>9.402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2882</td>\n",
       "      <td>-11.894</td>\n",
       "      <td>-21.136</td>\n",
       "      <td>True</td>\n",
       "      <td>9.242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>3008</td>\n",
       "      <td>54.419</td>\n",
       "      <td>17.509</td>\n",
       "      <td>True</td>\n",
       "      <td>36.910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>4904</td>\n",
       "      <td>10.582</td>\n",
       "      <td>-2.038</td>\n",
       "      <td>True</td>\n",
       "      <td>12.620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>6505</td>\n",
       "      <td>12.983</td>\n",
       "      <td>-15.217</td>\n",
       "      <td>True</td>\n",
       "      <td>28.201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>8046</td>\n",
       "      <td>159.351</td>\n",
       "      <td>68.624</td>\n",
       "      <td>True</td>\n",
       "      <td>90.727</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   stock  trading ror  buyhold ror   win   diff\n",
       "2   1301        3.236        1.733  True  1.503\n",
       "3   1303       -4.599       -5.455  True  0.855\n",
       "4   1326       -6.508      -15.459  True  8.951\n",
       "6   1590       -1.864      -10.803  True  8.939\n",
       "10  2308       15.592        7.447  True  8.145\n",
       "11  2317       -9.375      -23.537  True 14.162\n",
       "12  2324       -6.794      -10.664  True  3.869\n",
       "15  2357       -3.453      -16.727  True 13.274\n",
       "19  2408       23.088        6.923  True 16.165\n",
       "21  2412        7.273        2.326  True  4.947\n",
       "22  2603       -0.003      -26.720  True 26.717\n",
       "23  2609      -41.057      -41.855  True  0.798\n",
       "24  2615        2.601       -6.801  True  9.402\n",
       "28  2882      -11.894      -21.136  True  9.242\n",
       "36  3008       54.419       17.509  True 36.910\n",
       "39  4904       10.582       -2.038  True 12.620\n",
       "43  6505       12.983      -15.217  True 28.201\n",
       "44  8046      159.351       68.624  True 90.727"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock</th>\n",
       "      <th>trading ror</th>\n",
       "      <th>buyhold ror</th>\n",
       "      <th>win</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1101</td>\n",
       "      <td>0.528</td>\n",
       "      <td>40.739</td>\n",
       "      <td>False</td>\n",
       "      <td>-40.211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1216</td>\n",
       "      <td>2.719</td>\n",
       "      <td>10.417</td>\n",
       "      <td>False</td>\n",
       "      <td>-7.698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1402</td>\n",
       "      <td>11.056</td>\n",
       "      <td>11.381</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2002</td>\n",
       "      <td>-6.573</td>\n",
       "      <td>-3.239</td>\n",
       "      <td>False</td>\n",
       "      <td>-3.335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2207</td>\n",
       "      <td>21.890</td>\n",
       "      <td>90.251</td>\n",
       "      <td>False</td>\n",
       "      <td>-68.361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2303</td>\n",
       "      <td>-6.414</td>\n",
       "      <td>15.439</td>\n",
       "      <td>False</td>\n",
       "      <td>-21.852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2327</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>47.903</td>\n",
       "      <td>False</td>\n",
       "      <td>-47.906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2330</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>42.366</td>\n",
       "      <td>False</td>\n",
       "      <td>-42.368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2379</td>\n",
       "      <td>16.461</td>\n",
       "      <td>115.596</td>\n",
       "      <td>False</td>\n",
       "      <td>-99.135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2382</td>\n",
       "      <td>-6.550</td>\n",
       "      <td>3.877</td>\n",
       "      <td>False</td>\n",
       "      <td>-10.427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2395</td>\n",
       "      <td>9.816</td>\n",
       "      <td>40.465</td>\n",
       "      <td>False</td>\n",
       "      <td>-30.650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2409</td>\n",
       "      <td>-25.002</td>\n",
       "      <td>-18.623</td>\n",
       "      <td>False</td>\n",
       "      <td>-6.379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2801</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>45.499</td>\n",
       "      <td>False</td>\n",
       "      <td>-45.502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2880</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>43.875</td>\n",
       "      <td>False</td>\n",
       "      <td>-43.878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2881</td>\n",
       "      <td>-10.705</td>\n",
       "      <td>-9.020</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2884</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>67.801</td>\n",
       "      <td>False</td>\n",
       "      <td>-67.804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2885</td>\n",
       "      <td>0.571</td>\n",
       "      <td>46.377</td>\n",
       "      <td>False</td>\n",
       "      <td>-45.806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2886</td>\n",
       "      <td>0.373</td>\n",
       "      <td>26.446</td>\n",
       "      <td>False</td>\n",
       "      <td>-26.073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2887</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>10.413</td>\n",
       "      <td>False</td>\n",
       "      <td>-10.416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2891</td>\n",
       "      <td>1.999</td>\n",
       "      <td>9.535</td>\n",
       "      <td>False</td>\n",
       "      <td>-7.536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2892</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>23.664</td>\n",
       "      <td>False</td>\n",
       "      <td>-23.667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2912</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>5.739</td>\n",
       "      <td>False</td>\n",
       "      <td>-5.742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>3034</td>\n",
       "      <td>76.335</td>\n",
       "      <td>90.435</td>\n",
       "      <td>False</td>\n",
       "      <td>-14.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>3045</td>\n",
       "      <td>-2.466</td>\n",
       "      <td>3.704</td>\n",
       "      <td>False</td>\n",
       "      <td>-6.169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>4938</td>\n",
       "      <td>-5.445</td>\n",
       "      <td>-5.132</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>5880</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>32.214</td>\n",
       "      <td>False</td>\n",
       "      <td>-32.217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>6415</td>\n",
       "      <td>38.260</td>\n",
       "      <td>38.282</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>8454</td>\n",
       "      <td>-1.182</td>\n",
       "      <td>31.818</td>\n",
       "      <td>False</td>\n",
       "      <td>-33.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>9910</td>\n",
       "      <td>2.023</td>\n",
       "      <td>57.143</td>\n",
       "      <td>False</td>\n",
       "      <td>-55.120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   stock  trading ror  buyhold ror    win    diff\n",
       "0   1101        0.528       40.739  False -40.211\n",
       "1   1216        2.719       10.417  False  -7.698\n",
       "5   1402       11.056       11.381  False  -0.324\n",
       "7   2002       -6.573       -3.239  False  -3.335\n",
       "8   2207       21.890       90.251  False -68.361\n",
       "9   2303       -6.414       15.439  False -21.852\n",
       "13  2327       -0.003       47.903  False -47.906\n",
       "14  2330       -0.003       42.366  False -42.368\n",
       "16  2379       16.461      115.596  False -99.135\n",
       "17  2382       -6.550        3.877  False -10.427\n",
       "18  2395        9.816       40.465  False -30.650\n",
       "20  2409      -25.002      -18.623  False  -6.379\n",
       "25  2801       -0.003       45.499  False -45.502\n",
       "26  2880       -0.003       43.875  False -43.878\n",
       "27  2881      -10.705       -9.020  False  -1.685\n",
       "29  2884       -0.003       67.801  False -67.804\n",
       "30  2885        0.571       46.377  False -45.806\n",
       "31  2886        0.373       26.446  False -26.073\n",
       "32  2887       -0.003       10.413  False -10.416\n",
       "33  2891        1.999        9.535  False  -7.536\n",
       "34  2892       -0.003       23.664  False -23.667\n",
       "35  2912       -0.003        5.739  False  -5.742\n",
       "37  3034       76.335       90.435  False -14.100\n",
       "38  3045       -2.466        3.704  False  -6.169\n",
       "40  4938       -5.445       -5.132  False  -0.314\n",
       "41  5880       -0.003       32.214  False -32.217\n",
       "42  6415       38.260       38.282  False  -0.022\n",
       "45  8454       -1.182       31.818  False -33.001\n",
       "46  9910        2.023       57.143  False -55.120"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 34.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "file = f'performance_{time_slide}x22_3d.xlsx'\n",
    "save_path = os.path.join(gd_root,ror_root,file)\n",
    "trading_df.to_excel(save_path)\n",
    "\n",
    "display(trading_df[trading_df['win']==True])\n",
    "display(trading_df[trading_df['win']==False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "5AG6YEQj0LYT",
    "0fkh200L0eeD"
   ],
   "name": "Program Trading Simulation.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "interpreter": {
   "hash": "ecf5722fdaf1897a315d257d89d94520bfcaa453217d5becf09b39e73618b0de"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
